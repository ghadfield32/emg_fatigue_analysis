{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***** Adjust Input and output folders at the end of each module after if __name__****\n",
    "tip: for ease, keep the names the same and control H to replace the path when you do change it so any other of the same path change as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emg Load and Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Read sensor data with metadata and update column names with sensor group and mode identifiers\n",
    "########################################\n",
    "def read_sensor_data_with_metadata(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Reads the sensor CSV file that contains metadata in the first five lines,\n",
    "    a header in line 5, a sample rate row in line 6, and sensor data from line 7 onward.\n",
    "    \n",
    "    For updated data (with multiple sensors: e.g., FDS, FCU, FCR), it parses:\n",
    "      - Line 3: Sensor group identifiers (e.g., \"FDS (81770), , ... , FCU (81728), ..., FCR (81745)\")\n",
    "      - Line 4: Sensor mode information for each group.\n",
    "    \n",
    "    It then updates the header (line 5) by appending the sensor group to each column name.\n",
    "    The sensor mode is loaded into metadata but not appended to the column name.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the CSV file.\n",
    "      debug (bool): If True, prints detailed debug output.\n",
    "    \n",
    "    Returns:\n",
    "      df (pd.DataFrame): DataFrame with sensor data, updated column names, metadata columns, and a \"Timestamp\" column.\n",
    "      metadata (dict): Dictionary of parsed metadata.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        all_lines = f.readlines()\n",
    "    \n",
    "    metadata = {}\n",
    "    # --- Parse first 3 lines (common for both formats) ---\n",
    "    # Line 0: Application\n",
    "    line = all_lines[0].strip()\n",
    "    if ',' in line:\n",
    "        key, value = line.split(',', 1)\n",
    "        metadata[key.strip().rstrip(':')] = value.strip()\n",
    "    else:\n",
    "        metadata['Application'] = line\n",
    "\n",
    "    # Line 1: Date/Time\n",
    "    line = all_lines[1].strip()\n",
    "    if ',' in line:\n",
    "        key, value = line.split(',', 1)\n",
    "        metadata[key.strip().rstrip(':')] = value.strip()\n",
    "    else:\n",
    "        metadata['Date/Time'] = line\n",
    "\n",
    "    # Line 2: Collection Length (seconds)\n",
    "    line = all_lines[2].strip()\n",
    "    if ',' in line:\n",
    "        key, value = line.split(',', 1)\n",
    "        metadata[key.strip().rstrip(':')] = value.strip()\n",
    "    else:\n",
    "        metadata['Collection Length (seconds)'] = line\n",
    "\n",
    "    # --- Determine dataset type (updated or legacy) ---\n",
    "    sensor_group_line = all_lines[3].strip()\n",
    "    sensor_mode_line = all_lines[4].strip()\n",
    "    if ',' in sensor_group_line and len(sensor_group_line.split(',')) > 1:\n",
    "        # Updated dataset detected\n",
    "        sensor_group_tokens = [token.strip() for token in sensor_group_line.split(',')]\n",
    "        # Propagate non-empty values forward\n",
    "        sensor_groups = []\n",
    "        last = None\n",
    "        for token in sensor_group_tokens:\n",
    "            if token:\n",
    "                last = token\n",
    "            sensor_groups.append(last if last is not None else \"\")\n",
    "        # Similarly for sensor modes (line 4)\n",
    "        sensor_mode_tokens = [token.strip() for token in sensor_mode_line.split(',')]\n",
    "        sensor_modes = []\n",
    "        last_mode = None\n",
    "        for token in sensor_mode_tokens:\n",
    "            if token:\n",
    "                last_mode = token\n",
    "            sensor_modes.append(last_mode if last_mode is not None else \"\")\n",
    "        # Store these in metadata\n",
    "        metadata['SensorGroups'] = sensor_groups\n",
    "        metadata['SensorModes'] = sensor_modes\n",
    "        if debug:\n",
    "            print(f\"[read_sensor_data_with_metadata] SensorGroups: {sensor_groups}\")\n",
    "            print(f\"[read_sensor_data_with_metadata] SensorModes: {sensor_modes}\")\n",
    "    else:\n",
    "        # Legacy dataset: use line 3 and 4 as single values.\n",
    "        metadata['Sensor'] = sensor_group_line\n",
    "        metadata['Sensor Mode'] = sensor_mode_line\n",
    "\n",
    "    # --- Header row for sensor data is on line 5 in both cases ---\n",
    "    header_line = all_lines[5].strip()\n",
    "    original_col_names = [col.strip() for col in header_line.split(',')]\n",
    "    \n",
    "    # If updated dataset, update column names by appending only sensor group.\n",
    "    if 'SensorGroups' in metadata:\n",
    "        if len(metadata['SensorGroups']) >= len(original_col_names):\n",
    "            new_col_names = []\n",
    "            for i, col in enumerate(original_col_names):\n",
    "                group = metadata['SensorGroups'][i]\n",
    "                new_col_names.append(f\"{col} - {group}\")\n",
    "            if debug:\n",
    "                print(\"[read_sensor_data_with_metadata] New column names set (updated dataset, sensor group only).\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"[read_sensor_data_with_metadata] Warning: Not enough sensor group entries; using original column names.\")\n",
    "            new_col_names = original_col_names\n",
    "    else:\n",
    "        new_col_names = original_col_names\n",
    "\n",
    "    # Read the sensor data (starting at line 7)\n",
    "    data_str = ''.join(all_lines[7:])\n",
    "    df = pd.read_csv(StringIO(data_str), header=None, names=new_col_names)\n",
    "\n",
    "    # Add metadata columns to the DataFrame (except sensor group and mode lists)\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['SensorGroups', 'SensorModes']:\n",
    "            df[key] = value\n",
    "\n",
    "    # Create a running Timestamp column\n",
    "    collection_length = float(metadata.get('Collection Length (seconds)', 0))\n",
    "    start_time = pd.to_datetime(metadata.get('Date/Time', None))\n",
    "    num_samples = len(df)\n",
    "    time_offsets = np.linspace(0, collection_length, num_samples)\n",
    "    df['Timestamp'] = start_time + pd.to_timedelta(time_offsets, unit='s')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[read_sensor_data_with_metadata] Final DataFrame shape: {df.shape}\")\n",
    "        print(f\"[read_sensor_data_with_metadata] Final column names: {df.columns.tolist()}\")\n",
    "    else:\n",
    "        print(\"read_sensor_data_with_metadata completed.\")\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compute EMG extreme flag using a fixed time window\n",
    "########################################\n",
    "def compute_emg_extreme_flag_window(df, window_time=1.3, column='EMG 1 (mV)', \n",
    "                                    threshold_high=1.0, threshold_low=-0.5, debug=False):\n",
    "    \"\"\"\n",
    "    Computes a flag for each row indicating whether, within a fixed time window\n",
    "    around the current row, there is at least one EMG value above 'threshold_high' and\n",
    "    one below 'threshold_low'.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing a 'Timestamp' column.\n",
    "      window_time (float): Time window in seconds.\n",
    "      column (str): Column name with EMG values.\n",
    "      threshold_high (float): High threshold.\n",
    "      threshold_low (float): Low threshold.\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      pd.Series: Series of 0/1 flags.\n",
    "    \"\"\"\n",
    "    time_diffs = df['Timestamp'].diff().dropna().dt.total_seconds()\n",
    "    median_dt = time_diffs.median() if not time_diffs.empty else 0\n",
    "    frame_count = int(round(window_time / median_dt)) if median_dt > 0 else 0\n",
    "    if debug:\n",
    "        print(f\"[compute_emg_extreme_flag_window] Using a time window of {window_time} sec (~{frame_count} frames)\")\n",
    "    \n",
    "    flags = []\n",
    "    timestamps = df['Timestamp']\n",
    "    values = df[column]\n",
    "    for idx, current_time in timestamps.items():\n",
    "        start_time = current_time - pd.Timedelta(seconds=window_time)\n",
    "        end_time = current_time + pd.Timedelta(seconds=window_time)\n",
    "        window_mask = (timestamps >= start_time) & (timestamps <= end_time)\n",
    "        window_values = values[window_mask]\n",
    "        flag = int((window_values > threshold_high).any() and (window_values < threshold_low).any())\n",
    "        flags.append(flag)\n",
    "    \n",
    "    flag_series = pd.Series(flags, index=df.index)\n",
    "    if debug:\n",
    "        print(f\"[compute_emg_extreme_flag_window] Output flags shape: {flag_series.shape}\")\n",
    "    else:\n",
    "        print(\"compute_emg_extreme_flag_window completed.\")\n",
    "    return flag_series\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compute EMG extreme flag using a dynamic time window\n",
    "########################################\n",
    "def compute_emg_extreme_flag_dynamic_window(df, column='EMG 1 (mV)', threshold_high=1.0, \n",
    "                                              threshold_low=-0.5, debug=False):\n",
    "    \"\"\"\n",
    "    Computes a dynamic extreme flag for each row by first determining a fixed-window flag,\n",
    "    then adjusting the time window based on the nearest extreme events.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing a 'Timestamp' column.\n",
    "      column (str): Column name with EMG values.\n",
    "      threshold_high (float): High threshold.\n",
    "      threshold_low (float): Low threshold.\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      pd.Series: Series of 0/1 flags.\n",
    "    \"\"\"\n",
    "    fixed_flags = compute_emg_extreme_flag_window(df, window_time=1.3, column=column, \n",
    "                                                   threshold_high=threshold_high, threshold_low=threshold_low, debug=debug)\n",
    "    extreme_times = df.loc[fixed_flags == 1, 'Timestamp']\n",
    "    extreme_time_array = extreme_times.sort_values().values  # numpy array of timestamps\n",
    "    \n",
    "    new_flags = []\n",
    "    dynamic_windows = []  # store δ (in seconds) for each row\n",
    "    timestamps = df['Timestamp']\n",
    "    values = df[column]\n",
    "    \n",
    "    for idx, current_time in timestamps.items():\n",
    "        current_time_np = np.datetime64(current_time)\n",
    "        pos = np.searchsorted(extreme_time_array, current_time_np)\n",
    "        prev_extreme = extreme_time_array[pos - 1] if pos > 0 else None\n",
    "        next_extreme = extreme_time_array[pos] if pos < len(extreme_time_array) else None\n",
    "        \n",
    "        if prev_extreme is not None and next_extreme is not None:\n",
    "            delta_prev = (current_time_np - prev_extreme).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "            delta_next = (next_extreme - current_time_np).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "            delta_sec = min(delta_prev, delta_next)\n",
    "        elif prev_extreme is not None:\n",
    "            delta_sec = (current_time_np - prev_extreme).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "        elif next_extreme is not None:\n",
    "            delta_sec = (next_extreme - current_time_np).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "        else:\n",
    "            delta_sec = 0\n",
    "        dynamic_windows.append(delta_sec)\n",
    "        \n",
    "        start_time = current_time - pd.Timedelta(seconds=delta_sec)\n",
    "        end_time = current_time + pd.Timedelta(seconds=delta_sec)\n",
    "        window_mask = (timestamps >= start_time) & (timestamps <= end_time)\n",
    "        window_values = values[window_mask]\n",
    "        flag = int((window_values > threshold_high).any() and (window_values < threshold_low).any())\n",
    "        new_flags.append(flag)\n",
    "    \n",
    "    new_flags_series = pd.Series(new_flags, index=df.index)\n",
    "    avg_dynamic_window = np.mean(dynamic_windows) if dynamic_windows else 0\n",
    "    if debug:\n",
    "        print(f\"[compute_emg_extreme_flag_dynamic_window] Average dynamic window size: {avg_dynamic_window:.2f} sec\")\n",
    "    else:\n",
    "        print(\"compute_emg_extreme_flag_dynamic_window completed.\")\n",
    "    return new_flags_series\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Build global min/max dictionary\n",
    "########################################\n",
    "def build_global_min_max(df, columns_to_analyze, debug=False):\n",
    "    \"\"\"\n",
    "    Builds and returns a dictionary mapping each column to its global min and max.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame.\n",
    "      columns_to_analyze (list): List of column names.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      dict: Dictionary with min and max for each column.\n",
    "    \"\"\"\n",
    "    global_dict = {}\n",
    "    for col in columns_to_analyze:\n",
    "        global_dict[col] = {'min': df[col].min(), 'max': df[col].max()}\n",
    "    if debug:\n",
    "        print(f\"[build_global_min_max] Global min/max for columns: {global_dict}\")\n",
    "    else:\n",
    "        print(\"build_global_min_max completed.\")\n",
    "    return global_dict\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compute window metrics\n",
    "########################################\n",
    "def compute_window_metrics(window_df, columns_to_analyze, global_min_max=None, debug=False):\n",
    "    \"\"\"\n",
    "    Computes metrics (average, min, max) for a given window of data.\n",
    "    \n",
    "    Parameters:\n",
    "      window_df (pd.DataFrame): DataFrame slice.\n",
    "      columns_to_analyze (list): List of column names.\n",
    "      global_min_max (dict, optional): Dictionary for global min/max comparison.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      dict: Dictionary of computed metrics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for col in columns_to_analyze:\n",
    "        w_min = window_df[col].min()\n",
    "        w_max = window_df[col].max()\n",
    "        w_avg = window_df[col].mean()\n",
    "        stats[f'avg_{col}'] = w_avg\n",
    "        stats[f'min_{col}'] = w_min\n",
    "        stats[f'max_{col}'] = w_max\n",
    "        if global_min_max is not None and col in global_min_max:\n",
    "            g_min = global_min_max[col]['min']\n",
    "            g_max = global_min_max[col]['max']\n",
    "            stats[f'is_global_min_{col}'] = (w_min == g_min)\n",
    "            stats[f'is_global_max_{col}'] = (w_max == g_max)\n",
    "    if debug:\n",
    "        print(f\"[compute_window_metrics] Computed stats: {stats}\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Analyze spikes in a given column\n",
    "########################################\n",
    "def analyze_spikes(df, col, window=50, global_min_max=None, debug=False):\n",
    "    \"\"\"\n",
    "    Analyzes spikes in a given column using a specified window.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame.\n",
    "      col (str): Column name.\n",
    "      window (int): Window size (number of rows) around the spike.\n",
    "      global_min_max (dict, optional): Dictionary for global min/max.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with computed spike metrics.\n",
    "    \"\"\"\n",
    "    spike_flag_col = f'{col}_spike_flag'\n",
    "    flagged_indices = df.index[df[spike_flag_col] == 1]\n",
    "    results = []\n",
    "    columns_to_analyze = [\n",
    "        'EMG 1 (mV)', 'ACC X (G)', 'ACC Y (G)', 'ACC Z (G)',\n",
    "        'GYRO X (deg/s)', 'GYRO Y (deg/s)', 'GYRO Z (deg/s)'\n",
    "    ]\n",
    "    \n",
    "    for idx in flagged_indices:\n",
    "        start_idx = max(0, idx - window)\n",
    "        end_idx = min(len(df) - 1, idx + window)\n",
    "        window_df = df.loc[start_idx:end_idx]\n",
    "        window_stats = compute_window_metrics(window_df, columns_to_analyze, global_min_max=global_min_max, debug=debug)\n",
    "        window_stats['spike_index'] = idx\n",
    "        window_stats['spike_column'] = col\n",
    "        window_stats['spike_value'] = df.loc[idx, col]\n",
    "        window_stats['window_start'] = start_idx\n",
    "        window_stats['window_end'] = end_idx\n",
    "        results.append(window_stats)\n",
    "    if debug:\n",
    "        print(f\"[analyze_spikes] Processed {len(flagged_indices)} spikes for column {col}.\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compare spike windows from EMG and ACC/GYRO\n",
    "########################################\n",
    "def compare_spike_windows(emg_spikes_df, acc_gyro_spikes_df, debug=False):\n",
    "    \"\"\"\n",
    "    Compares spike windows from EMG and ACC/GYRO and returns merged information.\n",
    "    \n",
    "    Parameters:\n",
    "      emg_spikes_df (pd.DataFrame): DataFrame from analyze_spikes for EMG.\n",
    "      acc_gyro_spikes_df (pd.DataFrame): DataFrame from analyze_spikes for ACC/GYRO.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, emg_row in emg_spikes_df.iterrows():\n",
    "        emg_win_start = emg_row['window_start']\n",
    "        emg_win_end = emg_row['window_end']\n",
    "        overlapping_spikes = acc_gyro_spikes_df[\n",
    "            (acc_gyro_spikes_df['spike_index'] >= emg_win_start) &\n",
    "            (acc_gyro_spikes_df['spike_index'] <= emg_win_end)\n",
    "        ]\n",
    "        for j, spike_row in overlapping_spikes.iterrows():\n",
    "            merged_dict = {\n",
    "                'emg_spike_index': emg_row['spike_index'],\n",
    "                'emg_spike_value': emg_row['spike_value'],\n",
    "                'acc_gyro_spike_index': spike_row['spike_index'],\n",
    "                'acc_gyro_spike_column': spike_row['spike_column'],\n",
    "                'emg_window_avg': emg_row['avg_EMG 1 (mV)'],\n",
    "                'acc_window_avg': spike_row.get('avg_ACC X (G)', None)\n",
    "            }\n",
    "            rows.append(merged_dict)\n",
    "    if debug:\n",
    "        print(f\"[compare_spike_windows] Merged {len(rows)} overlapping spike events.\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Mark throwing motion based on extreme flag windows\n",
    "########################################\n",
    "def mark_throwing_motion(df, extreme_flag_col='EMG_extreme_flag', window_time=1.3, debug=False):\n",
    "    \"\"\"\n",
    "    Marks rows as part of the throwing motion based on extreme flag events.\n",
    "    \n",
    "    For each row where the specified extreme_flag_col is 1, mark all rows within ±(window_time/2) seconds \n",
    "    of that event's timestamp as part of the throwing motion by setting a new column 'ThrowingMotion' to 1.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame with a 'Timestamp' column.\n",
    "      extreme_flag_col (str): The column name that holds the extreme flag.\n",
    "      window_time (float): Total duration (in seconds) for the throwing motion window.\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Copy of the DataFrame with an added 'ThrowingMotion' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['ThrowingMotion'] = 0\n",
    "    half_window = window_time / 2  # e.g., 0.65 seconds for a 1.3-second window\n",
    "    \n",
    "    # Debug: Show the number of extreme events.\n",
    "    extreme_events = df.loc[df[extreme_flag_col] == 1, 'Timestamp']\n",
    "    if debug:\n",
    "        print(f\"[mark_throwing_motion] Found {len(extreme_events)} extreme events. Using half window = {half_window} sec.\")\n",
    "    \n",
    "    # Mark rows within the window of each extreme event.\n",
    "    for t in extreme_events:\n",
    "        start = t - pd.Timedelta(seconds=half_window)\n",
    "        end = t + pd.Timedelta(seconds=half_window)\n",
    "        mask = (df['Timestamp'] >= start) & (df['Timestamp'] <= end)\n",
    "        df.loc[mask, 'ThrowingMotion'] = 1\n",
    "        if debug:\n",
    "            print(f\"[mark_throwing_motion] Marking event at {t} (window: {start} to {end}).\")\n",
    "    \n",
    "    if debug:\n",
    "        total_marked = df['ThrowingMotion'].sum()\n",
    "        print(f\"[mark_throwing_motion] Total rows marked as ThrowingMotion: {total_marked}\")\n",
    "    else:\n",
    "        print(\"mark_throwing_motion completed.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Process a single CSV file\n",
    "########################################\n",
    "def process_file(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Processes a single sensor CSV file:\n",
    "      - Reads the file and its metadata.\n",
    "      - Performs cleaning and type conversion.\n",
    "      - Computes various flags and metrics.\n",
    "      - Marks throwing motion.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the CSV file.\n",
    "      debug (bool): If True, prints detailed debug output.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n[process_file] Processing file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Step 1: Read data and metadata.\n",
    "    df, metadata = read_sensor_data_with_metadata(file_path, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] DataFrame shape after reading: {df.shape}\")\n",
    "    else:\n",
    "        print(\"Data read completed.\")\n",
    "\n",
    "    # Step 2: Display minimal summary if in debug mode.\n",
    "    if debug:\n",
    "        print(f\"[process_file] Descriptive Statistics:\\n{df.describe()}\")\n",
    "        print(f\"[process_file] Data types:\\n{df.dtypes}\")\n",
    "    else:\n",
    "        print(\"Basic summary displayed.\")\n",
    "\n",
    "    # Step 3: Dynamically identify numeric sensor columns.\n",
    "    base_names = ['ACC X (G)', 'ACC Y (G)', 'ACC Z (G)', \n",
    "                  'GYRO X (deg/s)', 'GYRO Y (deg/s)', 'GYRO Z (deg/s)']\n",
    "    numeric_cols = []\n",
    "    for base in base_names:\n",
    "        matches = [col for col in df.columns if col.startswith(base)]\n",
    "        numeric_cols.extend(matches)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Identified numeric sensor columns: {numeric_cols}\")\n",
    "\n",
    "    # Clean data: Remove rows with blank numeric values.\n",
    "    mask = df[numeric_cols].apply(lambda col: col.astype(str).str.strip() == '').any(axis=1)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Rows with blank numeric values: {mask.sum()}\")\n",
    "    df = df[~mask]\n",
    "\n",
    "    # Convert identified numeric columns to numeric type.\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "        except Exception as e:\n",
    "            print(f\"[process_file] Error converting column {col}: {e}\")\n",
    "            raise\n",
    "    if debug:\n",
    "        print(f\"[process_file] Data shape after cleaning: {df.shape}\")\n",
    "\n",
    "    # Step 4: (Optional) Subset data; here we use the full dataset.\n",
    "    print(f\"[process_file] Data subset: {df.shape[0]} rows (full data used).\")\n",
    "    \n",
    "    # (Optional) Compute overall min/max summary.\n",
    "    min_max_df = pd.DataFrame({'min': df.min(), 'max': df.max()})\n",
    "    if debug:\n",
    "        print(f\"[process_file] Overall min/max summary:\\n{min_max_df}\")\n",
    "    else:\n",
    "        print(\"Min/Max summary computed.\")\n",
    "\n",
    "    # Step 5: Create spike flags for ACC/GYRO columns.\n",
    "    for col in numeric_cols:\n",
    "        spike_flag_col = f'{col}_spike_flag'\n",
    "        df[spike_flag_col] = ((df[col] > 1) | (df[col] < -0.5)).astype(int)\n",
    "    print(\"Spike flags for ACC/GYRO created.\")\n",
    "\n",
    "    # Create spike flag for EMG (value > 1.0).\n",
    "    emg_base = 'EMG 1 (mV)'\n",
    "    emg_matches = [col for col in df.columns if col.startswith(emg_base)]\n",
    "    if emg_matches:\n",
    "        emg_col = emg_matches[0]\n",
    "    else:\n",
    "        raise KeyError(f\"No column found starting with '{emg_base}'\")\n",
    "    emg_spike_flag_col = f'{emg_col}_spike_flag'\n",
    "    df[emg_spike_flag_col] = (df[emg_col] > 1.0).astype(int)\n",
    "    \n",
    "    # Additional EMG flags.\n",
    "    df['EMG_high_flag'] = (df[emg_col] > 1.0).astype(int)\n",
    "    df['EMG_low_flag'] = (df[emg_col] < -0.5).astype(int)\n",
    "    if debug:\n",
    "        print(f\"[process_file] EMG_high_flag, EMG_low_flag added. Count >1.0: {df['EMG_high_flag'].sum()}, \"\n",
    "              f\"Count <-0.5: {df['EMG_low_flag'].sum()}\")\n",
    "\n",
    "    # Step 6: Compute fixed-window extreme flag for EMG.\n",
    "    df['EMG_extreme_flag'] = compute_emg_extreme_flag_window(df, window_time=1.3, column=emg_col, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Fixed-window extreme flag count: {df['EMG_extreme_flag'].sum()}\")\n",
    "\n",
    "    # Step 7: Count unique extreme events in fixed window.\n",
    "    unique_extreme_count = ((df['EMG_extreme_flag'] == 1) &\n",
    "                            (df['EMG_extreme_flag'].shift(1).fillna(0) != 1)).sum()\n",
    "    if debug:\n",
    "        print(f\"[process_file] Unique extreme events (fixed window): {unique_extreme_count}\")\n",
    "\n",
    "    # Step 8: Compute dynamic-window extreme flag for EMG.\n",
    "    df['EMG_extreme_flag_dynamic'] = compute_emg_extreme_flag_dynamic_window(df, column=emg_col, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Dynamic-window extreme flag count: {df['EMG_extreme_flag_dynamic'].sum()}\")\n",
    "        unique_dynamic_extreme_count = ((df['EMG_extreme_flag_dynamic'] == 1) &\n",
    "                                        (df['EMG_extreme_flag_dynamic'].shift(1).fillna(0) != 1)).sum()\n",
    "        print(f\"[process_file] Unique extreme events (dynamic window): {unique_dynamic_extreme_count}\")\n",
    "\n",
    "    # Step 9: Mark throwing motion based on fixed-window extreme flags.\n",
    "    df = mark_throwing_motion(df, extreme_flag_col='EMG_extreme_flag', window_time=1.3, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] ThrowingMotion rows count: {df['ThrowingMotion'].sum()}\")\n",
    "\n",
    "    print(\"File processing completed.\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "########################################\n",
    "# Main function: Process all files in a folder and output a single Parquet file\n",
    "########################################\n",
    "def main(debug=False, input_folder='./data/raw/', output_file='./data/processed/processed_pitch_data.parquet'):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the specified folder, stacks them into one DataFrame,\n",
    "    and writes the output to a Parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      input_folder (str): Folder containing the CSV files.\n",
    "      output_file (str): Path for the output Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: Final processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure input folder exists.\n",
    "    if not os.path.isdir(input_folder):\n",
    "        raise FileNotFoundError(f\"Input folder '{input_folder}' does not exist.\")\n",
    "    \n",
    "    # Find all CSV files in the folder.\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the input folder.\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[main] Found {len(csv_files)} CSV files in '{input_folder}'.\")\n",
    "    else:\n",
    "        print(f\"Found {len(csv_files)} CSV file(s).\")\n",
    "\n",
    "    processed_dfs = []\n",
    "    for file in csv_files:\n",
    "        df = process_file(file, debug=debug)\n",
    "        # Optionally add a column to indicate source file.\n",
    "        df['SourceFile'] = os.path.basename(file)\n",
    "        processed_dfs.append(df)\n",
    "    \n",
    "    # Stack all DataFrames (row-wise).\n",
    "    final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    if debug:\n",
    "        print(f\"[main] Final stacked DataFrame shape: {final_df.shape}\")\n",
    "    else:\n",
    "        print(\"All files processed and stacked.\")\n",
    "\n",
    "    # Save final DataFrame to Parquet.\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    final_df.to_parquet(output_file, index=False)\n",
    "    print(f\"Final processed data saved to: {output_file}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Run the module when executed as a script.\n",
    "if __name__ == \"__main__\":\n",
    "    # Set debug=True for detailed output, or False for minimal output.\n",
    "    processed_df = main(\n",
    "        debug=True,\n",
    "        input_folder='../../data/raw/three_sensored_emg_data/',         # Specify your input folder path here.\n",
    "        output_file='../../data/processed/emg_pitch_data_processed.parquet'  # Specify your output file path here.\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granular Biomechanics Dataset from Theia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Check scipy version\n",
    "import scipy\n",
    "print(f\"SciPy version: {scipy.__version__}\")\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "\n",
    "\n",
    "# Get the current working directory as a Path object\n",
    "base_path = Path(os.getcwd())\n",
    "\n",
    "# Move two folders up using the 'parents' attribute\n",
    "env_path = base_path.parents[1] / '.env'\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "print(env_path)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_database_connection():\n",
    "    \"\"\"Create and return a database connection to the 'theia_pitching_db'.\"\"\"\n",
    "    return mysql.connector.connect(\n",
    "        host=os.getenv(\"DB_HOST\"),\n",
    "        user=os.getenv(\"DB_USER\"),\n",
    "        password=os.getenv(\"DB_PASSWORD\"),\n",
    "        database=os.getenv(\"DB_DATABASE\")\n",
    "    )\n",
    "\n",
    "def check_data_completeness(df, table_name, key_columns):\n",
    "    \"\"\"\n",
    "    Check for missing/incomplete data in a given DataFrame.\n",
    "    Logs the sum of nulls per column, unique non-null counts, and data types.\n",
    "    If a 'time' column is present, calculates basic time gap statistics.\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\nChecking data completeness for {table_name}:\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        logger.warning(f\"Null values found in {table_name}:\")\n",
    "        logger.warning(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        logger.info(\"No null values found.\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        unique_count = df[col].nunique(dropna=True)\n",
    "        col_type = df[col].dtype\n",
    "        logger.info(f\"Column '{col}': Type = {col_type}, Unique non-null values = {unique_count}\")\n",
    "    \n",
    "    if 'time' in df.columns:\n",
    "        time_stats = df.groupby('session_trial')['time'].apply(lambda x: x.diff().describe())\n",
    "        logger.info(f\"Time series statistics for {table_name} (averaged across sessions):\\n{time_stats.mean()}\")\n",
    "\n",
    "from scipy.signal import welch\n",
    "\n",
    "def calculate_spectral_features(signal_window, fs=100):\n",
    "    \"\"\"\n",
    "    Calculate spectral features for a given window of signal data using Welch's method.\n",
    "    Handles NaN values in the input signal by replacing them with 0.\n",
    "    \n",
    "    Parameters:\n",
    "      signal_window (array-like): 1D array of signal values (e.g., valgus_torque over a time window)\n",
    "      fs (int): Sampling frequency in Hz (default is 100)\n",
    "      \n",
    "    Returns:\n",
    "      dict: A dictionary containing:\n",
    "            - 'peak_freq': Frequency with the maximum power spectral density.\n",
    "            - 'total_power': Total power computed via integration of the PSD.\n",
    "    \"\"\"\n",
    "    # Replace NaNs with 0 in the signal\n",
    "    clean_signal = np.nan_to_num(signal_window, nan=0.0)\n",
    "    freqs, psd = welch(clean_signal, fs=fs)\n",
    "    peak_freq = freqs[np.argmax(psd)]\n",
    "    total_power = np.trapz(psd, freqs)\n",
    "    return {\n",
    "        'peak_freq': peak_freq,\n",
    "        'total_power': total_power\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_granular_time_series_data(filter_type='LAST_DAY', specific_date=None, specific_month=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Retrieves granular, frame-level time series data from 'theia_pitching_db'\n",
    "    with flexible date filtering options and returns all key metrics for injury analysis.\n",
    "    \n",
    "    Updates include:\n",
    "      - Converting the 'time' column from Decimal to float to prevent type conflicts.\n",
    "      - Using millisecond resolution for ongoing_timestamp.\n",
    "      - Adding the pitch phase marker as defined in the query.\n",
    "    \"\"\"\n",
    "    conn = get_database_connection()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Build the dynamic filtering part of the date_filtered_sessions CTE.\n",
    "    date_filter_cte = \"\"\"\n",
    "    WITH date_filtered_sessions AS (\n",
    "        SELECT\n",
    "            t.session_trial,\n",
    "            t.trial,\n",
    "            t.time AS trial_time,\n",
    "            TIMESTAMP(s.date, t.time) AS session_datetime,\n",
    "            t.pitch_type,\n",
    "            t.handedness,\n",
    "            s.date,\n",
    "            s.session,\n",
    "            s.level,\n",
    "            s.lab,\n",
    "            s.height_meters,\n",
    "            s.mass_kilograms,\n",
    "            u.name AS athlete_name,\n",
    "            u.dob AS athlete_dob,\n",
    "            u.traq AS athlete_traq\n",
    "        FROM `trials` t\n",
    "        JOIN `sessions` s ON t.session = s.session\n",
    "        JOIN `users` u ON s.user = u.user\n",
    "        WHERE \n",
    "    \"\"\"\n",
    "    \n",
    "    # Append the appropriate date filter based on the filter_type\n",
    "    if filter_type == 'LAST_DAY':\n",
    "        date_filter_cte += \"s.date = (SELECT MAX(date) FROM sessions)\"\n",
    "    elif filter_type == 'LAST_5_DAYS':\n",
    "        date_filter_cte += \"s.date >= DATE_SUB((SELECT MAX(date) FROM sessions), INTERVAL 4 DAY)\"\n",
    "    elif filter_type == 'LAST_MONTH':\n",
    "        date_filter_cte += \"s.date >= DATE_SUB((SELECT MAX(date) FROM sessions), INTERVAL 30 DAY)\"\n",
    "    elif filter_type == 'SPECIFIC_DATE':\n",
    "        if not specific_date:\n",
    "            raise ValueError(\"specific_date is required for SPECIFIC_DATE filter_type\")\n",
    "        date_filter_cte += f\"s.date = '{specific_date}'\"\n",
    "    elif filter_type == 'SPECIFIC_MONTH':\n",
    "        if not specific_month:\n",
    "            raise ValueError(\"specific_month is required for SPECIFIC_MONTH filter_type\")\n",
    "        date_filter_cte += f\"DATE_FORMAT(s.date, '%Y-%m') = '{specific_month}'\"\n",
    "    elif filter_type == 'DATE_RANGE':\n",
    "        if not start_date or not end_date:\n",
    "            raise ValueError(\"Both start_date and end_date are required for DATE_RANGE filter_type\")\n",
    "        date_filter_cte += f\"s.date BETWEEN '{start_date}' AND '{end_date}'\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid filter_type: {filter_type}\")\n",
    "    \n",
    "    date_filter_cte += \")\"\n",
    "    \n",
    "    # CTE to retrieve event markers for the pitch phases\n",
    "    pitch_phases_cte = \"\"\"\n",
    "    , pitch_phases AS (\n",
    "        SELECT \n",
    "            e.session_trial,\n",
    "            e.PKH_time,\n",
    "            e.FP_v5_time,\n",
    "            e.MER_time,\n",
    "            e.`BR_time` as ball_release_time,\n",
    "            e.`i_BR` as ball_release_frame,\n",
    "            e.MAD_time\n",
    "        FROM `events` e\n",
    "        INNER JOIN date_filtered_sessions dfs \n",
    "            ON e.session_trial = dfs.session_trial\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # CTE for frame-level joint data with added phase_marker\n",
    "    frame_level_data_cte = \"\"\"\n",
    "    , frame_level_data AS (\n",
    "        SELECT \n",
    "            ja.session_trial,\n",
    "            ja.time,\n",
    "            ja.shoulder_angle_x,\n",
    "            ja.shoulder_angle_y,\n",
    "            ja.shoulder_angle_z,\n",
    "            ja.elbow_angle_x,\n",
    "            ja.elbow_angle_y,\n",
    "            ja.elbow_angle_z,\n",
    "            ja.torso_angle_x,\n",
    "            ja.torso_angle_y,\n",
    "            ja.torso_angle_z,\n",
    "            ja.pelvis_angle_x,\n",
    "            ja.pelvis_angle_y,\n",
    "            ja.pelvis_angle_z,\n",
    "            COALESCE(jv.shoulder_velo_x, 0.0) AS shoulder_velo_x,\n",
    "            COALESCE(jv.shoulder_velo_y, 0.0) AS shoulder_velo_y,\n",
    "            COALESCE(jv.shoulder_velo_z, 0.0) AS shoulder_velo_z,\n",
    "            jv.elbow_velo_x,\n",
    "            jv.elbow_velo_y,\n",
    "            jv.elbow_velo_z,\n",
    "            jv.torso_velo_x,\n",
    "            jv.torso_velo_y,\n",
    "            jv.torso_velo_z,\n",
    "            ABS(ja.torso_angle_z - ja.pelvis_angle_z) AS trunk_pelvis_dissociation,\n",
    "            pp.ball_release_time,\n",
    "            -- Determine pitch phase using event markers\n",
    "            CASE \n",
    "                WHEN ja.time <= pp.PKH_time THEN 'Wind-Up'\n",
    "                WHEN ja.time <= pp.FP_v5_time THEN 'Stride'\n",
    "                WHEN ja.time <= pp.MER_time THEN 'Arm Cocking'\n",
    "                WHEN ja.time <= pp.ball_release_time THEN 'Arm Acceleration'\n",
    "                WHEN ja.time <= pp.MAD_time THEN 'Arm Deceleration'\n",
    "                ELSE 'Follow Through'\n",
    "            END AS pitch_phase,\n",
    "            pp.PKH_time AS phase_marker  -- added key phase marker from events\n",
    "        FROM `joint_angles` ja\n",
    "        INNER JOIN date_filtered_sessions dfs ON ja.session_trial = dfs.session_trial\n",
    "        INNER JOIN `joint_velos` jv ON ja.session_trial = jv.session_trial AND ja.time = jv.time\n",
    "        LEFT JOIN pitch_phases pp ON ja.session_trial = pp.session_trial\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build final query with additional joins and updated timestamp calculation.\n",
    "    query = (\n",
    "        date_filter_cte +\n",
    "        pitch_phases_cte +\n",
    "        frame_level_data_cte +\n",
    "        \"\"\"\n",
    "    SELECT \n",
    "        dfs.athlete_name,\n",
    "        dfs.athlete_dob,\n",
    "        dfs.athlete_traq,\n",
    "        dfs.height_meters,\n",
    "        dfs.mass_kilograms,\n",
    "        dfs.level AS athlete_level,\n",
    "        dfs.date AS session_date,\n",
    "        dfs.trial_time AS session_time,\n",
    "        dfs.lab,\n",
    "        dfs.session,\n",
    "        dfs.trial,\n",
    "        dfs.pitch_type,\n",
    "        dfs.handedness,\n",
    "        -- Updated ongoing_timestamp calculation with millisecond resolution\n",
    "        DATE_ADD(dfs.session_datetime, INTERVAL fld.time SECOND) AS ongoing_timestamp,\n",
    "        fld.*,\n",
    "        en.shoulder_energy_transfer,\n",
    "        en.shoulder_energy_generation,\n",
    "        en.elbow_energy_transfer,\n",
    "        en.elbow_energy_generation,\n",
    "        en.lead_knee_energy_transfer,\n",
    "        en.lead_knee_energy_generation,\n",
    "\n",
    "        /*-- Force plate data\n",
    "        fp.lead_force_x,\n",
    "        fp.lead_force_y,\n",
    "        fp.lead_force_z,\n",
    "        fp.lead_force_mag,\n",
    "        fp.rear_force_x,\n",
    "        fp.rear_force_y,\n",
    "        fp.rear_force_z,\n",
    "        fp.rear_force_mag,*/\n",
    "\n",
    "        jf.elbow_force_x,\n",
    "        jf.elbow_force_y,\n",
    "        jf.elbow_force_z,\n",
    "        jf.shoulder_upper_arm_force_x,\n",
    "        jf.shoulder_upper_arm_force_y,\n",
    "        jf.shoulder_upper_arm_force_z,\n",
    "        COALESCE(jm.elbow_moment_x, 0.0) AS elbow_moment_x,\n",
    "        COALESCE(jm.elbow_moment_y, 0.0) AS elbow_moment_y,\n",
    "        COALESCE(jm.elbow_moment_z, 0.0) AS elbow_moment_z,\n",
    "        jm.shoulder_thorax_moment_x,\n",
    "        jm.shoulder_thorax_moment_y,\n",
    "        jm.shoulder_thorax_moment_z,\n",
    "        p.pitch_speed_mph,\n",
    "        p.max_shoulder_internal_rotational_velo,\n",
    "        p.elbow_varus_moment,\n",
    "        ll.forearm_length\n",
    "    FROM frame_level_data fld\n",
    "    LEFT JOIN `energetics` en ON fld.session_trial = en.session_trial AND fld.time = en.time\n",
    "    LEFT JOIN `force_plates` fp ON fld.session_trial = fp.session_trial AND fld.time = fp.time\n",
    "    LEFT JOIN `joint_forces` jf ON fld.session_trial = jf.session_trial AND fld.time = jf.time\n",
    "    LEFT JOIN `joint_moments` jm ON fld.session_trial = jm.session_trial AND fld.time = jm.time\n",
    "    LEFT JOIN `poi` p ON fld.session_trial = p.session_trial\n",
    "    LEFT JOIN `limb_lengths` ll ON fld.session_trial = ll.session_trial\n",
    "    JOIN date_filtered_sessions dfs ON fld.session_trial = dfs.session_trial\n",
    "    ORDER BY \n",
    "        dfs.date,\n",
    "        dfs.trial_time,\n",
    "        fld.session_trial,\n",
    "        fld.time;\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Executing updated query with filter_type: {filter_type}\")\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    # Convert 'time' column to numeric (float) to resolve Decimal type issues from SQL import.\n",
    "    df['time'] = pd.to_numeric(df['time'], errors='coerce').fillna(0)\n",
    "    \n",
    "    logger.info(f\"Query returned {len(df)} rows and {len(df.columns)} columns.\")\n",
    "    return df\n",
    "\n",
    "    \n",
    "def trapz_integration(x):\n",
    "    return np.trapz(x)\n",
    "\n",
    "def calculate_dynamic_phase_weights(group):\n",
    "    \"\"\"\n",
    "    Calculate dynamic phase weights for a given group while handling Decimal type conflicts.\n",
    "    \n",
    "    Steps:\n",
    "      1. Convert 'time' to float to avoid Decimal arithmetic issues.\n",
    "      2. Compute duration and perform a placeholder normalization.\n",
    "      3. Compute torque integration normalization (also placeholder logic).\n",
    "      4. Return a dictionary with the pitch phase as the key and the calculated weight as the value.\n",
    "    \"\"\"\n",
    "    # Ensure 'time' column is float to avoid type conflicts (Decimal vs. float)\n",
    "    time_vals = group['time'].astype(float)\n",
    "    \n",
    "    # Calculate the duration and normalize (placeholder: returns 1 if nonzero)\n",
    "    duration = time_vals.max() - time_vals.min()\n",
    "    duration_norm = duration / duration if duration != 0 else 0\n",
    "    \n",
    "    # Calculate torque integration normalization (placeholder: returns 1 if nonzero)\n",
    "    torque_int = trapz_integration(group['valgus_torque'].astype(float))\n",
    "    torque_int_norm = torque_int / torque_int if torque_int != 0 else 0\n",
    "    \n",
    "    shoulder_mean = group['shoulder_ang_velo'].mean()\n",
    "    \n",
    "    # Combine metrics with placeholder weights (update as needed for proper normalization)\n",
    "    weight = (\n",
    "        0.4 * torque_int_norm + \n",
    "        0.3 * duration_norm +\n",
    "        0.2 * (group['valgus_torque'].max() / group['valgus_torque'].max() if group['valgus_torque'].max() != 0 else 0) +\n",
    "        0.1 * (shoulder_mean / shoulder_mean if shoulder_mean != 0 else 0)\n",
    "    )\n",
    "    return {group['pitch_phase'].iloc[0]: weight}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_phase_aware_cumulative(group):\n",
    "    \"\"\"Dynamic phase weight implementation\"\"\"\n",
    "    # Calculate session-specific weights\n",
    "    phase_weights = calculate_dynamic_phase_weights(group)\n",
    "    \n",
    "    cum_values = []\n",
    "    current_phase = None\n",
    "    current_cum = 0\n",
    "    \n",
    "    for idx, row in group.iterrows():\n",
    "        if row['pitch_phase'] != current_phase:\n",
    "            current_phase = row['pitch_phase']\n",
    "            current_cum = 0\n",
    "            \n",
    "        weight = phase_weights.get(current_phase, 1.0)\n",
    "        dt = row['time_diff_lead']\n",
    "        d_torque = row['torque_diff_lead']\n",
    "        \n",
    "        current_cum += d_torque * weight * dt\n",
    "        cum_values.append(current_cum)\n",
    "    \n",
    "    return pd.Series(cum_values, index=group.index)\n",
    "\n",
    "def validate_phase_weights(df):\n",
    "    \"\"\"Ensure automated weights match biomechanical expectations\"\"\"\n",
    "    expected_pattern = {\n",
    "        'Wind-Up': (0.9, 1.2),\n",
    "        'Arm Acceleration': (1.9, 2.3),  # Should be highest\n",
    "        'Arm Deceleration': (1.7, 2.0)\n",
    "    }\n",
    "    \n",
    "    sample_weights = df.groupby('session_trial').apply(\n",
    "        lambda g: calculate_dynamic_phase_weights(g)\n",
    "    ).explode().groupby(level=1).mean()\n",
    "    \n",
    "    violations = []\n",
    "    for phase, bounds in expected_pattern.items():\n",
    "        if not (bounds[0] <= sample_weights[phase] <= bounds[1]):\n",
    "            violations.append(f\"{phase}: {sample_weights[phase]:.2f}\")\n",
    "    \n",
    "    if violations:\n",
    "        logger.error(f\"Phase weight violations:\\n{', '.join(violations)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def compute_cumulative_exposure(group):\n",
    "    \"\"\"\n",
    "    Compute the cumulative valgus torque exposure for a group (session_trial).\n",
    "    This function logs key intermediate outputs and handles any NaN values in the torque array\n",
    "    by replacing them with 0.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing group: {group.name}\")\n",
    "    \n",
    "    # Log original ongoing_timestamp values and their data type.\n",
    "    logger.info(\"Original ongoing_timestamp values:\")\n",
    "    logger.info(group['ongoing_timestamp'].head(5))\n",
    "    logger.info(f\"Data type: {group['ongoing_timestamp'].dtype}\")\n",
    "    \n",
    "    try:\n",
    "        # Convert ongoing_timestamp to seconds (from ns)\n",
    "        times = group['ongoing_timestamp'].astype('int64') / 1e9\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting ongoing_timestamp in group {group.name}: {e}\")\n",
    "        raise\n",
    "    logger.info(\"Converted times (seconds):\")\n",
    "    logger.info(times.head(5))\n",
    "    \n",
    "    # Ensure no NaNs in time values\n",
    "    assert not times.isnull().any(), f\"NaN found in times for group {group.name}\"\n",
    "    \n",
    "    # Retrieve torque values and log them.\n",
    "    torque = group['valgus_torque'].values\n",
    "    logger.info(\"Torque values:\")\n",
    "    logger.info(torque[:5])\n",
    "    \n",
    "    # If any NaNs are found in torque, log a warning and replace them with 0.\n",
    "    if np.isnan(torque).any():\n",
    "        logger.warning(f\"NaN detected in torque for group {group.name}; replacing NaNs with 0.\")\n",
    "        torque = np.nan_to_num(torque, nan=0.0)\n",
    "    \n",
    "    try:\n",
    "        # Perform cumulative integration using the trapezoidal rule.\n",
    "        cum = cumulative_trapezoid(torque, times, initial=0)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during cumulative integration in group {group.name}: {e}\")\n",
    "        raise\n",
    "    logger.info(\"Cumulative integration result:\")\n",
    "    logger.info(cum[:5])\n",
    "    \n",
    "    return pd.Series(cum, index=group.index)\n",
    "\n",
    "\n",
    "def compute_armcock_acc_cumulative(group):\n",
    "    cum_values = []\n",
    "    current_cum = 0\n",
    "    phase_mask = group['pitch_phase'].isin(['Arm Cocking', 'Arm Acceleration'])\n",
    "    \n",
    "    for idx, (time, torque, phase) in enumerate(zip(group['ongoing_timestamp'], \n",
    "                                                    group['valgus_torque'], \n",
    "                                                    group['pitch_phase'])):\n",
    "        if not phase_mask.iloc[idx]:\n",
    "            current_cum = 0\n",
    "        else:\n",
    "            dt = (group['ongoing_timestamp'].iloc[idx] - \n",
    "                 group['ongoing_timestamp'].iloc[idx-1]).total_seconds() if idx > 0 else 0\n",
    "            current_cum += torque * dt\n",
    "        cum_values.append(current_cum)\n",
    "    \n",
    "    return pd.Series(cum_values, index=group.index)\n",
    "\n",
    "\n",
    "def process_valgus_features(df):\n",
    "    \"\"\"\n",
    "    Process the DataFrame to compute additional valgus torque features for ML.\n",
    "    Enhancements include:\n",
    "      - Phase-specific cumulative torque calculation.\n",
    "      - Velocity-scaled torque feature.\n",
    "      - Peak torque identification.\n",
    "      - Enhanced data validation.\n",
    "      - Robust conversion of shoulder velocity columns to numeric.\n",
    "      - Creation of an alias for 'pitch_phase_biomech' to ensure compatibility with plotting code.\n",
    "    \"\"\"\n",
    "    # ----------------- Step A: Validate Input -----------------\n",
    "    logger.info(\"Validating ongoing_timestamp column before processing valgus features.\")\n",
    "    df['ongoing_timestamp'] = pd.to_datetime(df['ongoing_timestamp'], errors='raise')\n",
    "    \n",
    "    if 'pitch_phase' not in df.columns:\n",
    "        raise KeyError(\"Missing required column 'pitch_phase' for phase-aware calculations\")\n",
    "    \n",
    "    # ----------------- Step B: Compute Valgus Torque -----------------\n",
    "    df['valgus_torque'] = -pd.to_numeric(df['elbow_moment_z'], errors='coerce').fillna(0)\n",
    "    df['valgus_torque'] = df['valgus_torque'].mask(df['valgus_torque'] < 0, 0)\n",
    "    \n",
    "    df['ball_release_time'] = pd.to_numeric(df['ball_release_time'], errors='coerce')\n",
    "    # Time to Ball Release\n",
    "    df['time_to_br'] = df.groupby('session_trial').apply(\n",
    "        lambda g: g['ball_release_time'] - g['time']\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    df['time_diff_lead'] = df.groupby('session_trial')['ongoing_timestamp'].shift(-1) - df['ongoing_timestamp']\n",
    "    df['time_diff_lead'] = df['time_diff_lead'].dt.total_seconds()\n",
    "    df['torque_diff_lead'] = df.groupby('session_trial')['valgus_torque'].shift(-1) - df['valgus_torque']\n",
    "    \n",
    "    # Valgus Impulse: Cumulative sum of torque leading up to ball release\n",
    "    df['valgus_impulse'] = (df['valgus_torque'] * \n",
    "                           df['time_diff_lead']).cumsum()\n",
    "\n",
    "    # ----------------- Step C: Validate and Convert Shoulder Velocity Columns -----------------\n",
    "    vel_cols = ['shoulder_velo_x', 'shoulder_velo_y', 'shoulder_velo_z']\n",
    "    logger.info(\"\\nShoulder velocity column dtypes before conversion:\")\n",
    "    logger.info(df[vel_cols].dtypes)\n",
    "    logger.info(\"\\nSample shoulder velocity values before conversion:\")\n",
    "    logger.info(df[vel_cols].head(3))\n",
    "    \n",
    "    for col in vel_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    logger.info(\"\\nPost-conversion null counts for velocity columns:\")\n",
    "    logger.info(df[vel_cols].isnull().sum())\n",
    "    logger.info(\"\\nPost-conversion dtypes for velocity columns:\")\n",
    "    logger.info(df[vel_cols].dtypes)\n",
    "    \n",
    "    # ----------------- Step D: Compute Angular Velocity and Velocity-Scaled Torque -----------------\n",
    "    df['shoulder_ang_velo'] = np.sqrt(\n",
    "        df['shoulder_velo_x']**2 + \n",
    "        df['shoulder_velo_y']**2 + \n",
    "        df['shoulder_velo_z']**2\n",
    "    )\n",
    "    df['velocity_scaled_torque'] = df['valgus_torque'] * df['shoulder_ang_velo']\n",
    "    \n",
    "    # ----------------- Step E: Compute Temporal Derivatives -----------------\n",
    "    df['torque_derivative'] = df.groupby('session_trial')['valgus_torque'].diff() / \\\n",
    "        df.groupby('session_trial')['ongoing_timestamp'].diff().dt.total_seconds()\n",
    "    \n",
    "\n",
    "    # ----------------- Step F: Apply Cumulative Calculations -----------------\n",
    "    df['phase_weighted_cumulative'] = df.groupby(\n",
    "        ['session_trial', 'pitch_phase'], \n",
    "        group_keys=False\n",
    "    ).apply(compute_phase_aware_cumulative)\n",
    "    \n",
    "    df['cumulative_valgus'] = df.groupby('session_trial', group_keys=False).apply(compute_cumulative_exposure)\n",
    "    \n",
    "    df['critical_phase'] = df['pitch_phase'].isin(['Arm Cocking', 'Arm Acceleration'])\n",
    "    df['cumulative_valgus_phase_armcock_acc'] = df.groupby('session_trial', group_keys=False).apply(compute_armcock_acc_cumulative)\n",
    "    \n",
    "    # ----------------- Step G: Identify Peak Torque -----------------\n",
    "    df['peak_torque_marker'] = df.groupby('session_trial')['valgus_torque'].transform(\n",
    "        lambda x: x == x.max()\n",
    "    )\n",
    "    \n",
    "    # Remove helper columns used in cumulative calculations\n",
    "    df.drop(columns=['time_diff_lead', 'torque_diff_lead'], inplace=True)\n",
    "    \n",
    "    # # ----------------- Step H: Create Alias for Pitch Phase -----------------\n",
    "    # # This ensures that downstream plotting (which expects 'pitch_phase_biomech') works properly.\n",
    "    # if 'pitch_phase_biomech' not in df.columns and 'pitch_phase' in df.columns:\n",
    "    #     df['pitch_phase_biomech'] = df['pitch_phase']\n",
    "    \n",
    "    logger.info(\"Finished processing valgus features. Sample of updated DataFrame:\")\n",
    "    logger.info(df.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_valgus_calculation(df):\n",
    "    \"\"\"\n",
    "    Validate that all required columns for valgus torque calculations are present and reasonable.\n",
    "    Raises an error if key columns are missing and logs a warning if more than 10% of the computed\n",
    "    valgus torque values are zero.\n",
    "    \"\"\"\n",
    "    required_columns = [\n",
    "        'elbow_moment_x', 'elbow_moment_y', 'elbow_moment_z',\n",
    "        'forearm_length', 'mass_kilograms', 'valgus_torque'\n",
    "    ]\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing valgus calculation columns: {missing}\")\n",
    "    \n",
    "    zero_moment = df[df['valgus_torque'] == 0]\n",
    "    if len(zero_moment) > 0.1 * len(df):\n",
    "        logger.warning(\">10% zero valgus torque values detected\")\n",
    "\n",
    "\n",
    "def validate_valgus_calculation(df):\n",
    "    \"\"\"\n",
    "    Validate that all required columns for valgus torque calculations are present and reasonable.\n",
    "    Raises an error if key columns are missing and logs a warning if more than 10% of the computed\n",
    "    valgus torque values are zero.\n",
    "    Also logs a null report for key torque columns and checks for unexpected negative values.\n",
    "    \"\"\"\n",
    "    required_columns = [\n",
    "        'elbow_moment_x', 'elbow_moment_y', 'elbow_moment_z',\n",
    "        'forearm_length', 'mass_kilograms', 'valgus_torque',\n",
    "        'shoulder_velo_x', 'shoulder_velo_y', 'shoulder_velo_z'\n",
    "    ]\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing valgus calculation columns: {missing}\")\n",
    "    \n",
    "    # New: Log null counts for key columns.\n",
    "    null_report = df[['elbow_moment_z', 'valgus_torque']].isnull().sum()\n",
    "    logger.info(f\"Null values report:\\n{null_report}\")\n",
    "    \n",
    "    # New: Check for negative torque values after masking.\n",
    "    if (df['valgus_torque'] < 0).any():\n",
    "        logger.error(\"Negative valgus_torque values detected after masking\")\n",
    "    \n",
    "    zero_moment = df[df['valgus_torque'] == 0]\n",
    "    if len(zero_moment) > 0.1 * len(df):\n",
    "        logger.warning(\">10% zero valgus torque values detected\")\n",
    "\n",
    "\n",
    "# ----------------------- Main Execution Block -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # logger.info(\"Requesting granular time series dataset with LAST_DAY filter...\")\n",
    "    # df_last_day = get_granular_time_series_data(filter_type='LAST_DAY')\n",
    "    # logger.info(\"LAST_DAY filter complete. Displaying head:\")\n",
    "    # print(df_last_day.head(10))\n",
    "    # logger.info(\"Displaying pitch phase data from last day:\")\n",
    "    # print(\"\\nPitch phase values:\")\n",
    "    # print(df_last_day['pitch_phase'].value_counts().sort_index())\n",
    "\n",
    "    # logger.info(\"Requesting granular time series dataset with LAST_5_DAYS filter...\")\n",
    "    # df_last_5 = get_granular_time_series_data(filter_type='LAST_5_DAYS')\n",
    "    # logger.info(\"LAST_5_DAYS filter complete. Displaying head:\")\n",
    "    # print(df_last_5.head(10))\n",
    "    \n",
    "    logger.info(\"Requesting granular time series dataset with SPECIFIC_DATE filter (2025-02-14)...\")\n",
    "    df_specific = get_granular_time_series_data(filter_type='SPECIFIC_DATE', specific_date='2025-02-14')\n",
    "    logger.info(\"SPECIFIC_DATE filter complete. Displaying head:\")\n",
    "    print(df_specific.head(10))\n",
    "    \n",
    "    # New test for DATE_RANGE filter\n",
    "    logger.info(\"Requesting granular time series dataset with DATE_RANGE filter (2025-02-14 to 2025-02-19)...\")\n",
    "    df_date_range = get_granular_time_series_data(\n",
    "        filter_type='DATE_RANGE', \n",
    "        start_date='2025-02-13', \n",
    "        end_date='2025-02-20'\n",
    "    )\n",
    "    logger.info(\"DATE_RANGE filter complete. Displaying head:\")\n",
    "    print(df_date_range.head(10))\n",
    "\n",
    "    # Process the dataset to compute valgus torque features\n",
    "    logger.info(\"Processing valgus torque features...\")\n",
    "    df_specific = process_valgus_features(df_specific)\n",
    "    df_date_range = process_valgus_features(df_date_range)\n",
    "    # Validate the computed valgus features\n",
    "    validate_valgus_calculation(df_specific)\n",
    "    \n",
    "    logger.info(\"\\nChecking ongoing_timestamp column:\")\n",
    "    logger.info(f\"Number of unique timestamps: {df_specific['ongoing_timestamp'].nunique()}\")\n",
    "    logger.info(\"\\nTimestamp range:\")\n",
    "    logger.info(f\"Earliest: {df_specific['ongoing_timestamp'].min()}\")\n",
    "    logger.info(f\"Latest: {df_specific['ongoing_timestamp'].max()}\")\n",
    "    logger.info(\"\\nSample of timestamps:\")\n",
    "    print(df_specific['ongoing_timestamp'].head())\n",
    "    \n",
    "    logger.info(\"\\nColumn information (including new valgus features):\")\n",
    "    for col in df_specific.columns:\n",
    "        logger.info(f\"\\nColumn: {col}\")\n",
    "        logger.info(f\"Data type: {df_specific[col].dtype}\")\n",
    "        logger.info(f\"Number of unique values: {df_specific[col].nunique()}\")\n",
    "        logger.info(f\"Number of null values: {df_specific[col].isnull().sum()}\")\n",
    "        if df_specific[col].dtype in ['object', 'category']:\n",
    "            logger.info(\"Sample unique values:\")\n",
    "            print(df_specific[col].unique()[:5])\n",
    "        elif df_specific[col].dtype in ['int64', 'float64']:\n",
    "            logger.info(\"Numeric summary:\")\n",
    "            print(df_specific[col].describe())\n",
    "    \n",
    "    #-----------------DROPPING FORCE PLATE COLUMNS (if no force plate data)--------------------------------\n",
    "    force_plate_cols = [col for col in df_specific.columns if 'force_' in col.lower()]\n",
    "    logger.info(f\"\\nDropping {len(force_plate_cols)} force plate columns:\")\n",
    "    for col in force_plate_cols:\n",
    "        logger.info(f\"- {col}\")\n",
    "    df_specific = df_specific.drop(columns=force_plate_cols)\n",
    "    logger.info(f\"Remaining columns after drop: {len(df_specific.columns)}\")\n",
    "    \n",
    "    # Save specific date dataframe to parquet\n",
    "    output_path = '../../data/processed/ml_datasets/granular/granular_joint_details.parquet'\n",
    "    logger.info(f\"Saving specific date data to {output_path}\")\n",
    "    # Log shape and basic checks before saving\n",
    "    logger.info(f\"\\nDataFrame shape: {df_specific.shape}\")\n",
    "    \n",
    "    logger.info(\"\\nBasic phase checks:\")\n",
    "    logger.info(f\"Number of zeros in pitch_phase: {(df_specific['pitch_phase'] == 0).sum()}\")\n",
    "    logger.info(f\"Number of nulls in pitch_phase: {df_specific['pitch_phase'].isnull().sum()}\")\n",
    "    \n",
    "    logger.info(\"\\nValue counts in pitch_phase:\")\n",
    "    logger.info(df_specific['pitch_phase'].value_counts())\n",
    "    \n",
    "    # Check for any null values across all columns\n",
    "    null_cols = df_specific.columns[df_specific.isnull().any()].tolist()\n",
    "    if len(null_cols) > 0:\n",
    "        logger.info(\"\\nColumns containing null values:\")\n",
    "        for col in null_cols:\n",
    "            logger.info(f\"- {col}: {df_specific[col].isnull().sum()} nulls\")\n",
    "    else:\n",
    "        logger.info(\"\\nNo null values found in any columns\")\n",
    "        \n",
    "    logger.info(\"\\nChecking for overlapping pitch phases, time gaps and phase durations per trial...\")\n",
    "    df_date_range.to_parquet(output_path)\n",
    "    logger.info(\"Save complete\")\n",
    "\n",
    "    # Check null sums and column list\n",
    "    logger.info(\"\\nChecking null values across all columns:\")\n",
    "    null_sums = df_specific.isnull().sum()\n",
    "    logger.info(\"\\nColumns with null values:\")\n",
    "    for col, null_count in null_sums[null_sums > 0].items():\n",
    "        logger.info(f\"- {col}: {null_count} nulls\")\n",
    "\n",
    "    logger.info(\"\\nFull column list:\")\n",
    "    for col in sorted(df_specific.columns):\n",
    "        logger.info(f\"- {col}\")\n",
    "\n",
    "\n",
    "    # Also show summary statistics per phase\n",
    "    logger.info(\"\\nSummary statistics of cumulative valgus per pitch phase:\")\n",
    "    phase_valgus_stats = df_specific.groupby('pitch_phase')['cumulative_valgus'].describe()\n",
    "    logger.info(phase_valgus_stats)\n",
    "    # Plot the progression of cumulative valgus over time\n",
    "    logger.info(\"\\nCreating plot of cumulative_valgus_phase_armcock_acc progression:\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Plot lines for each pitch phase\n",
    "    for phase in df_specific['pitch_phase'].unique():\n",
    "        phase_data = df_specific[df_specific['pitch_phase'] == phase]\n",
    "        plt.plot(phase_data['ongoing_timestamp'], \n",
    "                phase_data['cumulative_valgus_phase_armcock_acc'],\n",
    "                label=phase, alpha=0.7)\n",
    "        \n",
    "        # Add vertical lines for min and max timestamps\n",
    "        min_time = phase_data['ongoing_timestamp'].min()\n",
    "        max_time = phase_data['ongoing_timestamp'].max()\n",
    "        plt.axvline(x=min_time, color='gray', linestyle='--', alpha=0.3)\n",
    "        plt.axvline(x=max_time, color='gray', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Add text labels for min/max lines\n",
    "        y_pos = plt.ylim()[1]\n",
    "        plt.text(min_time, y_pos, f'{phase} start', rotation=90, verticalalignment='top')\n",
    "        plt.text(max_time, y_pos, f'{phase} end', rotation=90, verticalalignment='top')\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Cumulative Valgus (Phase Arm Cock Acc)')\n",
    "    plt.title('Progression of Cumulative Valgus by Pitch Phase')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also show summary statistics per phase\n",
    "    logger.info(\"\\nSummary statistics of cumulative_valgus_phase_armcock_acc per pitch phase:\")\n",
    "    phase_valgus_stats = df_date_range.groupby('pitch_phase')['cumulative_valgus_phase_armcock_acc'].describe()\n",
    "    logger.info(phase_valgus_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine EMG and Biomech\n",
    "\n",
    "Granular Datasets:\n",
    "Resample/interpolate the biomechanics dataset to join evenly onto the EMG data: No changes\n",
    "Bio dataset with EMG filtered out dataset: is_interpolated filter: filter for non interpolated data if you want to take away emg data for a bio dataset without interpolated added columns (will filter out EMG so they are on the bio frequency)\n",
    "EMG dataset with phases added on: create a interpolated column list so we can differ that from the non and create a EMG dataset with pitch phases added on (creating the simplistic EMG dataset with phases added on, no fake data involved and straight to the muscles dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module: emg_biomech_inner_join.py\n",
    "\n",
    "Goal:\n",
    "    To add biomech data to the EMG data after interpolating the biomech dataset\n",
    "    granular enough to provide a datapoint for each EMG metric. In addition to\n",
    "    interpolating the biomech metrics, we add an identifier column indicating\n",
    "    whether a row was interpolated (new) or originally present. We focus on the\n",
    "    inner join workflow, retaining only EMG rows that receive complete biomech data.\n",
    "\n",
    "Usage:\n",
    "    Run this module as a script to load, process, and join the datasets.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_biomech_data(biomech_path, debug=False):\n",
    "    \"\"\"\n",
    "    Load and prepare biomechanical data from a parquet file.\n",
    "    Ensures the datetime column is parsed.\n",
    "    \n",
    "    Parameters:\n",
    "        biomech_path (str): Path to the parquet file.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded biomechanical data.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(biomech_path)\n",
    "    if 'ongoing_timestamp' not in df.columns:\n",
    "        raise ValueError(\"Biomech data missing 'ongoing_timestamp' column.\")\n",
    "    df['ongoing_timestamp'] = pd.to_datetime(df['ongoing_timestamp'])\n",
    "    df['datetime'] = df['ongoing_timestamp']\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] load_biomech_data: DataFrame shape = {df.shape}\")\n",
    "        print(f\"[DEBUG] Datetime range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "        print(f\"[DEBUG] New columns: { {col: str(dtype) for col, dtype in df[['ongoing_timestamp', 'datetime']].dtypes.items()} }\")\n",
    "    else:\n",
    "        print(\"Biomech data loaded.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_emg_data(emg_path, debug=False):\n",
    "    \"\"\"\n",
    "    Load and prepare EMG data from a CSV file.\n",
    "    Parses datetime using either 'Date/Time' and/or 'Timestamp' columns.\n",
    "    \n",
    "    Parameters:\n",
    "        emg_path (str): Path to the CSV file.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded EMG data.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(emg_path)\n",
    "    if 'Date/Time' in df.columns and 'Timestamp' in df.columns:\n",
    "        df['Date/Time_parsed'] = pd.to_datetime(df['Date/Time'])\n",
    "        df['Timestamp_parsed'] = pd.to_datetime(df['Timestamp'])\n",
    "        df['emg_time'] = df['Timestamp']\n",
    "        df['datetime'] = df['Timestamp_parsed']\n",
    "    elif 'Date/Time' in df.columns:\n",
    "        df['emg_time'] = df['Date/Time']\n",
    "        df['datetime'] = pd.to_datetime(df['Date/Time'])\n",
    "    elif 'Timestamp' in df.columns:\n",
    "        df['emg_time'] = df['Timestamp']\n",
    "        df['datetime'] = pd.to_datetime(df['Timestamp'])\n",
    "    else:\n",
    "        raise ValueError(\"EMG data missing a datetime column ('Date/Time' or 'Timestamp').\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] load_emg_data: DataFrame shape = {df.shape}\")\n",
    "        print(f\"[DEBUG] Datetime range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "    else:\n",
    "        print(\"EMG data loaded.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_time_steps(df, debug=False):\n",
    "    \"\"\"\n",
    "    Compute time steps based on the datetime column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'time_step' column.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    df['time_step'] = df['datetime'].diff()\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] compute_time_steps: DataFrame shape = {df.shape}\")\n",
    "        # Print a summary of the new column\n",
    "        print(f\"[DEBUG] 'time_step' sample dtypes: {df['time_step'].dtype}, sample values: {df['time_step'].head(3).tolist()}\")\n",
    "    else:\n",
    "        print(\"Time steps computed.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_dataframes(biomech_df, emg_df, debug=False):\n",
    "    \"\"\"\n",
    "    Sort both DataFrames by datetime.\n",
    "    \n",
    "    Parameters:\n",
    "        biomech_df (pd.DataFrame): Biomech DataFrame.\n",
    "        emg_df (pd.DataFrame): EMG DataFrame.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Sorted biomech and EMG DataFrames.\n",
    "    \"\"\"\n",
    "    biomech_df = biomech_df.sort_values('datetime').reset_index(drop=True)\n",
    "    emg_df = emg_df.sort_values('datetime').reset_index(drop=True)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] sort_dataframes: Biomech shape = {biomech_df.shape}, EMG shape = {emg_df.shape}\")\n",
    "    else:\n",
    "        print(\"DataFrames sorted.\")\n",
    "    return biomech_df, emg_df\n",
    "\n",
    "\n",
    "def filter_biomech_by_emg_range(biomech_df, emg_df, buffer_minutes=30, debug=False):\n",
    "    \"\"\"\n",
    "    Filter the biomech DataFrame for each day in the EMG data so that only rows \n",
    "    falling within the EMG datetime range (plus a buffer) are retained.\n",
    "    \n",
    "    Parameters:\n",
    "        biomech_df (pd.DataFrame): Biomech DataFrame.\n",
    "        emg_df (pd.DataFrame): EMG DataFrame.\n",
    "        buffer_minutes (int): Minutes to buffer on each side.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered biomech DataFrame.\n",
    "    \"\"\"\n",
    "    biomech_df = biomech_df.copy()\n",
    "    emg_df = emg_df.copy()\n",
    "    # Add temporary date columns\n",
    "    biomech_df['date'] = biomech_df['datetime'].dt.date\n",
    "    emg_df['date'] = emg_df['datetime'].dt.date\n",
    "\n",
    "    filtered_list = []\n",
    "    for day in emg_df['date'].unique():\n",
    "        emg_day = emg_df[emg_df['date'] == day]\n",
    "        day_min = emg_day['datetime'].min()\n",
    "        day_max = emg_day['datetime'].max()\n",
    "        day_min_buffered = day_min - pd.Timedelta(minutes=buffer_minutes)\n",
    "        day_max_buffered = day_max + pd.Timedelta(minutes=buffer_minutes)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] filter_biomech_by_emg_range: Day {day} - EMG range: {day_min} to {day_max}, Buffered: {day_min_buffered} to {day_max_buffered}\")\n",
    "        biomech_day = biomech_df[(biomech_df['datetime'] >= day_min_buffered) & \n",
    "                                 (biomech_df['datetime'] <= day_max_buffered)]\n",
    "        filtered_list.append(biomech_day)\n",
    "    filtered_biomech = pd.concat(filtered_list, ignore_index=True)\n",
    "    filtered_biomech = filtered_biomech.drop(columns=['date'])\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] filter_biomech_by_emg_range: Filtered biomech shape = {filtered_biomech.shape}\")\n",
    "    else:\n",
    "        print(\"Biomech data filtered by EMG range.\")\n",
    "    return filtered_biomech\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_emg_integrity(original_emg, joined_df, join_description=\"join\", debug=False):\n",
    "    \"\"\"\n",
    "    Check that the joined DataFrame retains all EMG rows.\n",
    "    \n",
    "    Parameters:\n",
    "        original_emg (pd.DataFrame): Original EMG DataFrame.\n",
    "        joined_df (pd.DataFrame): Joined DataFrame.\n",
    "        join_description (str): Description of the join.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "    \"\"\"\n",
    "    original_count = original_emg.shape[0]\n",
    "    joined_count = joined_df.shape[0]\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] {join_description}: Original EMG rows = {original_count}, Joined rows = {joined_count}\")\n",
    "    if original_count != joined_count:\n",
    "        print(f\"[WARNING] EMG integrity check failed in {join_description}: original = {original_count}, joined = {joined_count}.\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] {join_description}: All EMG rows retained.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_interpolation_quality(resampled_df, original_biomech_df, emg_df, debug=False):\n",
    "    \"\"\"\n",
    "    Check the quality of interpolation by analyzing:\n",
    "    1. Distribution of distances to the nearest original biomech points.\n",
    "    2. Proportion of EMG points within various tolerance thresholds.\n",
    "    3. Statistical comparison of numeric values between original and interpolated rows.\n",
    "    \n",
    "    Parameters:\n",
    "        resampled_df (pd.DataFrame): The resampled/interpolated biomech DataFrame.\n",
    "        original_biomech_df (pd.DataFrame): The original biomech DataFrame.\n",
    "        emg_df (pd.DataFrame): The EMG DataFrame.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics about interpolation quality.\n",
    "    \"\"\"\n",
    "    if resampled_df.empty:\n",
    "        print(\"[WARNING] Empty resampled DataFrame, cannot check interpolation quality\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"\\n=== Interpolation Quality Check ===\")\n",
    "    \n",
    "    # Get sorted original biomech timestamps as a numpy array\n",
    "    original_times = original_biomech_df['datetime'].sort_values().values\n",
    "    # Filter the interpolated rows\n",
    "    interpolated_rows = resampled_df[resampled_df['is_interpolated']].copy()\n",
    "    \n",
    "    if interpolated_rows.empty:\n",
    "        print(\"[INFO] No interpolated rows to check\")\n",
    "        return {}\n",
    "    \n",
    "    distances_ns = []\n",
    "    CHUNK_SIZE = 1000  # Process in chunks to manage memory\n",
    "    \n",
    "    # Loop through the interpolated rows in chunks\n",
    "    for i in range(0, len(interpolated_rows), CHUNK_SIZE):\n",
    "        chunk = interpolated_rows.iloc[i:i+CHUNK_SIZE]\n",
    "        for ts in chunk['datetime']:\n",
    "            idx = np.searchsorted(original_times, ts)\n",
    "            if idx == 0:\n",
    "                distance = abs(int((ts - original_times[0]).total_seconds() * 1e9))\n",
    "            elif idx == len(original_times):\n",
    "                distance = abs(int((ts - original_times[-1]).total_seconds() * 1e9))\n",
    "            else:\n",
    "                distance = min(\n",
    "                    abs(int((ts - original_times[idx-1]).total_seconds() * 1e9)),\n",
    "                    abs(int((original_times[idx] - ts).total_seconds() * 1e9))\n",
    "                )\n",
    "            distances_ns.append(distance)\n",
    "    \n",
    "    # Convert distances from nanoseconds to milliseconds\n",
    "    distances_ms = np.array(distances_ns) / 1_000_000\n",
    "    \n",
    "    print(f\"[INFO] Distance statistics (ms) to nearest original biomech point:\")\n",
    "    print(f\"  Min: {distances_ms.min():.3f}, Max: {distances_ms.max():.3f}\")\n",
    "    print(f\"  Mean: {distances_ms.mean():.3f}, Median: {np.median(distances_ms):.3f}\")\n",
    "    print(f\"  Std Dev: {distances_ms.std():.3f}\")\n",
    "    \n",
    "    # Check proportions of points within several tolerance thresholds\n",
    "    tolerances = [1, 5, 10, 50, 100]  # in milliseconds\n",
    "    for tol in tolerances:\n",
    "        within_tol = (distances_ms <= tol).sum()\n",
    "        percent = (within_tol / len(distances_ms)) * 100\n",
    "        print(f\"  {within_tol} points ({percent:.2f}%) within {tol}ms of an original biomech point\")\n",
    "    \n",
    "    # Compare numeric columns between original and interpolated rows (if debug is True)\n",
    "    numeric_cols = resampled_df.select_dtypes(include=['number']).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['is_interpolated']]\n",
    "    \n",
    "    if debug and numeric_cols:\n",
    "        print(\"[DEBUG] Value comparison for selected numeric columns:\")\n",
    "        for col in numeric_cols[:5]:  # Limit to first 5 columns for brevity\n",
    "            original_values = resampled_df[~resampled_df['is_interpolated']][col]\n",
    "            interpolated_values = resampled_df[resampled_df['is_interpolated']][col]\n",
    "            \n",
    "            if original_values.empty or interpolated_values.empty:\n",
    "                continue\n",
    "                \n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    Original - Mean: {original_values.mean():.4f}, Std: {original_values.std():.4f}\")\n",
    "            print(f\"    Interpolated - Mean: {interpolated_values.mean():.4f}, Std: {interpolated_values.std():.4f}\")\n",
    "    \n",
    "    # Check how many EMG timestamps have a corresponding biomech row\n",
    "    emg_timestamps = set(emg_df['datetime'])\n",
    "    resampled_timestamps = set(resampled_df['datetime'])\n",
    "    common_timestamps = emg_timestamps.intersection(resampled_timestamps)\n",
    "    coverage = len(common_timestamps) / len(emg_timestamps) * 100\n",
    "    print(f\"[INFO] {len(common_timestamps)} of {len(emg_timestamps)} EMG timestamps ({coverage:.2f}%) have corresponding biomech data\")\n",
    "    \n",
    "    return {\n",
    "        'distances_ms': distances_ms,\n",
    "        'coverage_percent': coverage,\n",
    "        'interpolated_count': len(interpolated_rows),\n",
    "        'original_count': len(resampled_df) - len(interpolated_rows)\n",
    "    }\n",
    "\n",
    "\n",
    "def save_dataframe(df, out_path, step_name, debug=False):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a parquet file using the provided output path.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame to save.\n",
    "        out_path (str): Output file path.\n",
    "        step_name (str): Name of the processing step (for logging).\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "    \"\"\"\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] {step_name} saved to: {out_path}\")\n",
    "    else:\n",
    "        print(f\"{step_name} completed and saved.\")\n",
    "\n",
    "\n",
    "def deep_analysis(inner_df, debug=False):\n",
    "    \"\"\"\n",
    "    Provides a concise deep analysis of the inner join results.\n",
    "    \n",
    "    Parameters:\n",
    "        inner_df (pd.DataFrame): DataFrame resulting from the inner join.\n",
    "        debug (bool): If True, prints detailed analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Deep Analysis ===\")\n",
    "    print(f\"Inner join shape: {inner_df.shape}\")\n",
    "    null_counts = inner_df.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        print(\"[WARNING] Null values found in inner join:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        print(\"No null values found in inner join.\")\n",
    "    dup_count = inner_df.duplicated(subset=[\"datetime\"]).sum()\n",
    "    if dup_count:\n",
    "        print(f\"[WARNING] Inner join has {dup_count} duplicate datetime rows.\")\n",
    "    else:\n",
    "        print(\"No duplicate datetime rows in inner join.\")\n",
    "    print(\"[INFO] Summary statistics for time differences (seconds):\")\n",
    "    if \"time_difference\" in inner_df.columns:\n",
    "        print(inner_df[\"time_difference\"].dt.total_seconds().describe())\n",
    "\n",
    "\n",
    "def analyze_temporal_alignment(biomech_df, emg_df, debug=False):\n",
    "    \"\"\"\n",
    "    Analyze the temporal alignment between biomech and EMG data to identify viable join candidates.\n",
    "    \n",
    "    Parameters:\n",
    "        biomech_df (pd.DataFrame): Biomech DataFrame.\n",
    "        emg_df (pd.DataFrame): EMG DataFrame.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics about temporal alignment.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Temporal Alignment Analysis ===\")\n",
    "    \n",
    "    # Work on copies and add a 'date' column for daily analysis\n",
    "    biomech_df = biomech_df.copy()\n",
    "    emg_df = emg_df.copy()\n",
    "    biomech_df['date'] = biomech_df['datetime'].dt.date\n",
    "    emg_df['date'] = emg_df['datetime'].dt.date\n",
    "    \n",
    "    # Identify days present in each dataset and their intersection\n",
    "    biomech_days = set(biomech_df['date'])\n",
    "    emg_days = set(emg_df['date'])\n",
    "    common_days = biomech_days.intersection(emg_days)\n",
    "    \n",
    "    print(f\"[INFO] Days with biomech data: {len(biomech_days)}\")\n",
    "    print(f\"[INFO] Days with EMG data: {len(emg_days)}\")\n",
    "    print(f\"[INFO] Days with both data types: {len(common_days)}\")\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Common days: {sorted(common_days)}\")\n",
    "    \n",
    "    # Initialize counters and distance bins\n",
    "    counts = {\n",
    "        'total_emg': 0,\n",
    "        'viable_emg': 0,\n",
    "        'distance_stats': {\n",
    "            '1ms': 0,\n",
    "            '5ms': 0,\n",
    "            '10ms': 0,\n",
    "            '50ms': 0,\n",
    "            '100ms': 0,\n",
    "            '500ms': 0,\n",
    "            '1s': 0,\n",
    "            '5s': 0,\n",
    "            'other': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process each common day\n",
    "    for day in sorted(common_days):\n",
    "        day_biomech = biomech_df[biomech_df['date'] == day].sort_values('datetime')\n",
    "        day_emg = emg_df[emg_df['date'] == day].sort_values('datetime')\n",
    "        day_emg_count = len(day_emg)\n",
    "        counts['total_emg'] += day_emg_count\n",
    "        \n",
    "        # Skip if no biomech data for the day\n",
    "        if len(day_biomech) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Convert datetime objects to Unix timestamps (seconds since epoch)\n",
    "        # FIX: Use pandas methods directly to handle datetime64 objects properly\n",
    "        biomech_times = day_biomech['datetime'].values\n",
    "        biomech_unix_times = day_biomech['datetime'].map(pd.Timestamp.timestamp).values\n",
    "        \n",
    "        # Store original timestamps for calculating time differences later\n",
    "        biomech_timestamps = day_biomech['datetime'].values\n",
    "        \n",
    "        closest_distances_ms = []\n",
    "        chunk_size = 1000\n",
    "        for i in range(0, len(day_emg), chunk_size):\n",
    "            emg_chunk = day_emg.iloc[i:i+chunk_size]\n",
    "            for _, emg_row in emg_chunk.iterrows():\n",
    "                emg_ts = emg_row['datetime']\n",
    "                emg_unix_ts = emg_ts.timestamp()\n",
    "                \n",
    "                # Search for the position in the sorted array using Unix timestamps\n",
    "                idx = np.searchsorted(biomech_unix_times, emg_unix_ts)\n",
    "                \n",
    "                if idx == 0:\n",
    "                    # EMG timestamp is before first biomech timestamp\n",
    "                    closest_biomech = biomech_timestamps[0]\n",
    "                    distance_ms = abs((emg_ts - pd.Timestamp(closest_biomech)).total_seconds() * 1000)\n",
    "                elif idx == len(biomech_timestamps):\n",
    "                    # EMG timestamp is after last biomech timestamp\n",
    "                    closest_biomech = biomech_timestamps[-1]\n",
    "                    distance_ms = abs((emg_ts - pd.Timestamp(closest_biomech)).total_seconds() * 1000)\n",
    "                else:\n",
    "                    # Find closest between previous and next biomech timestamps\n",
    "                    prev_biomech = biomech_timestamps[idx-1]\n",
    "                    next_biomech = biomech_timestamps[idx]\n",
    "                    prev_distance = abs((emg_ts - pd.Timestamp(prev_biomech)).total_seconds() * 1000)\n",
    "                    next_distance = abs((emg_ts - pd.Timestamp(next_biomech)).total_seconds() * 1000)\n",
    "                    distance_ms = min(prev_distance, next_distance)\n",
    "                \n",
    "                closest_distances_ms.append(distance_ms)\n",
    "                \n",
    "                # Categorize the distance into bins\n",
    "                if distance_ms <= 1:\n",
    "                    counts['distance_stats']['1ms'] += 1\n",
    "                    counts['viable_emg'] += 1\n",
    "                elif distance_ms <= 5:\n",
    "                    counts['distance_stats']['5ms'] += 1\n",
    "                elif distance_ms <= 10:\n",
    "                    counts['distance_stats']['10ms'] += 1\n",
    "                elif distance_ms <= 50:\n",
    "                    counts['distance_stats']['50ms'] += 1\n",
    "                elif distance_ms <= 100:\n",
    "                    counts['distance_stats']['100ms'] += 1\n",
    "                elif distance_ms <= 500:\n",
    "                    counts['distance_stats']['500ms'] += 1\n",
    "                elif distance_ms <= 1000:\n",
    "                    counts['distance_stats']['1s'] += 1\n",
    "                elif distance_ms <= 5000:\n",
    "                    counts['distance_stats']['5s'] += 1\n",
    "                else:\n",
    "                    counts['distance_stats']['other'] += 1\n",
    "        \n",
    "        if debug and closest_distances_ms:\n",
    "            print(f\"[DEBUG] Day {day}: {len(closest_distances_ms)} EMG points analyzed\")\n",
    "            print(f\"[DEBUG] Day {day}: Distance stats (ms) - Min: {min(closest_distances_ms):.2f}, Max: {max(closest_distances_ms):.2f}, Mean: {np.mean(closest_distances_ms):.2f}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Distance Distribution Summary:\")\n",
    "    for label, count in counts['distance_stats'].items():\n",
    "        pct = (count / counts['total_emg'] * 100) if counts['total_emg'] > 0 else 0\n",
    "        print(f\"  {label}: {count} points ({pct:.2f}%)\")\n",
    "    print(f\"\\n[INFO] Total EMG points: {counts['total_emg']}\")\n",
    "    viable_pct = (counts['viable_emg'] / counts['total_emg'] * 100) if counts['total_emg'] > 0 else 0\n",
    "    print(f\"[INFO] EMG points within 1ms of biomech: {counts['viable_emg']} ({viable_pct:.2f}%)\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def selective_resample_biomech_by_emg(biomech_df, emg_df, tolerance_ms=1, \n",
    "                                     max_analysis_distance_ms=5000,\n",
    "                                     categorical_cols=None,\n",
    "                                     categorical_numeric_cols=None,\n",
    "                                     debug=False):\n",
    "    \"\"\"\n",
    "    Selectively resample biomech data only for EMG timestamps that have nearby biomech data.\n",
    "    This version first identifies viable candidates (within a maximum analysis distance) and then\n",
    "    performs interpolation only for those specific timestamps.\n",
    "    \n",
    "    Parameters:\n",
    "        biomech_df (pd.DataFrame): Filtered biomech DataFrame.\n",
    "        emg_df (pd.DataFrame): EMG DataFrame.\n",
    "        tolerance_ms (int): Tolerance in milliseconds for interpolation.\n",
    "        max_analysis_distance_ms (int): Maximum distance (in ms) to consider for analysis.\n",
    "        categorical_cols (list, optional): Columns to treat as categorical.\n",
    "        categorical_numeric_cols (list, optional): Numeric columns to treat categorically.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Selectively resampled biomech DataFrame.\n",
    "    \"\"\"\n",
    "    # Set default categorical columns if not provided\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = [\n",
    "            'athlete_name', 'athlete_dob', 'athlete_traq',\n",
    "            'athlete_level', 'lab', 'pitch_type', 'handedness', 'session_date',\n",
    "            'height_meters', 'mass_kilograms',\n",
    "            'lab', 'session', 'trial', 'pitch_type', 'handedness',\n",
    "            'session_trial', 'pitch_speed_mph', 'date', 'time_step',\n",
    "            'pitch_phase', 'session_date', 'time', 'session_time'\n",
    "        ]\n",
    "    if categorical_numeric_cols is None:\n",
    "        categorical_numeric_cols = ['session', 'trial']\n",
    "\n",
    "    # Work on copies and add date columns for daily processing\n",
    "    biomech_df = biomech_df.copy()\n",
    "    emg_df = emg_df.copy()\n",
    "    biomech_df['date'] = biomech_df['datetime'].dt.date\n",
    "    emg_df['date'] = emg_df['datetime'].dt.date\n",
    "    \n",
    "    # Calculate analysis buffer (in ns) and interpolation buffer (in ns)\n",
    "    analysis_buffer_ns = max_analysis_distance_ms * 1_000_000\n",
    "    interp_buffer_ns = tolerance_ms * 1_000_000\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] selective_resample: Starting with {len(biomech_df)} biomech rows and {len(emg_df)} EMG rows\")\n",
    "        print(f\"[DEBUG] Biomech date range: {biomech_df['date'].min()} to {biomech_df['date'].max()}\")\n",
    "        print(f\"[DEBUG] EMG date range: {emg_df['date'].min()} to {emg_df['date'].max()}\")\n",
    "        print(f\"[DEBUG] Using interpolation tolerance of {tolerance_ms}ms ({interp_buffer_ns}ns)\")\n",
    "        print(f\"[DEBUG] Using analysis distance of {max_analysis_distance_ms}ms ({analysis_buffer_ns}ns)\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    interpolated_count = 0\n",
    "    exact_match_count = 0\n",
    "    outside_tolerance_count = 0\n",
    "    no_biomech_data_count = 0\n",
    "    resampled_list = []\n",
    "    \n",
    "    # Process only the common days\n",
    "    common_days = set(biomech_df['date']).intersection(set(emg_df['date']))\n",
    "    \n",
    "    for day in sorted(common_days):\n",
    "        emg_day = emg_df[emg_df['date'] == day]\n",
    "        biomech_day = biomech_df[biomech_df['date'] == day]\n",
    "        day_interpolated = 0\n",
    "        day_exact_match = 0\n",
    "        day_outside_tolerance = 0\n",
    "        day_no_biomech_data = 0\n",
    "        \n",
    "        if biomech_day.empty:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] selective_resample: No biomech data for day {day}. Skipping.\")\n",
    "            no_biomech_data_count += len(emg_day)\n",
    "            continue\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Day {day}: Processing {len(emg_day)} EMG rows and {len(biomech_day)} biomech rows\")\n",
    "        \n",
    "        biomech_day_sorted = biomech_day.sort_values('datetime')\n",
    "        emg_timestamps = emg_day['datetime'].sort_values().unique()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Day {day}: Found {len(emg_timestamps)} unique EMG timestamps\")\n",
    "        \n",
    "        day_results = []\n",
    "        original_timestamps = set(biomech_day_sorted['datetime'])\n",
    "        processed_count = 0\n",
    "        \n",
    "        for emg_ts in emg_timestamps:\n",
    "            processed_count += 1\n",
    "            if debug and processed_count % 10000 == 0:\n",
    "                print(f\"[DEBUG] Day {day}: Processed {processed_count}/{len(emg_timestamps)} EMG timestamps\")\n",
    "            \n",
    "            if emg_ts in original_timestamps:\n",
    "                exact_row = biomech_day_sorted[biomech_day_sorted['datetime'] == emg_ts].copy()\n",
    "                exact_row['is_interpolated'] = False\n",
    "                day_results.append(exact_row)\n",
    "                day_exact_match += 1\n",
    "                continue\n",
    "            \n",
    "            before_mask = biomech_day_sorted['datetime'] < emg_ts\n",
    "            after_mask = biomech_day_sorted['datetime'] > emg_ts\n",
    "            if not before_mask.any() or not after_mask.any():\n",
    "                day_no_biomech_data += 1\n",
    "                continue\n",
    "            \n",
    "            before_idx = before_mask.values.nonzero()[0][-1]\n",
    "            after_idx = after_mask.values.nonzero()[0][0]\n",
    "            before_ts = biomech_day_sorted.iloc[before_idx]['datetime']\n",
    "            after_ts = biomech_day_sorted.iloc[after_idx]['datetime']\n",
    "            \n",
    "            # Check if either gap is within the strict tolerance\n",
    "            time_diff_before_ns = abs(int((emg_ts - before_ts).total_seconds() * 1e9))\n",
    "            time_diff_after_ns = abs(int((after_ts - emg_ts).total_seconds() * 1e9))\n",
    "            if time_diff_before_ns > interp_buffer_ns and time_diff_after_ns > interp_buffer_ns:\n",
    "                day_outside_tolerance += 1\n",
    "                continue\n",
    "            \n",
    "            before_row = biomech_day_sorted.iloc[before_idx].copy()\n",
    "            after_row = biomech_day_sorted.iloc[after_idx].copy()\n",
    "            new_row = before_row.copy()\n",
    "            new_row['datetime'] = emg_ts\n",
    "            new_row['is_interpolated'] = True\n",
    "            \n",
    "            total_time_diff_ns = int((after_ts - before_ts).total_seconds() * 1e9)\n",
    "            for col in biomech_day_sorted.columns:\n",
    "                if col in ['datetime', 'is_interpolated', 'date'] or col in categorical_cols:\n",
    "                    continue\n",
    "                if col in categorical_numeric_cols:\n",
    "                    new_row[col] = before_row[col] if time_diff_before_ns <= time_diff_after_ns else after_row[col]\n",
    "                else:\n",
    "                    try:\n",
    "                        before_val = pd.to_numeric(before_row[col])\n",
    "                        after_val = pd.to_numeric(after_row[col])\n",
    "                        if total_time_diff_ns > 0:\n",
    "                            position = time_diff_before_ns / total_time_diff_ns\n",
    "                            new_row[col] = before_val + position * (after_val - before_val)\n",
    "                    except (ValueError, TypeError):\n",
    "                        new_row[col] = before_row[col] if time_diff_before_ns <= time_diff_after_ns else after_row[col]\n",
    "            day_results.append(pd.DataFrame([new_row]))\n",
    "            day_interpolated += 1\n",
    "        \n",
    "        if day_results:\n",
    "            day_df = pd.concat(day_results, ignore_index=True)\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Day {day}: Created {len(day_df)} biomech rows (exact: {day_exact_match}, interp: {day_interpolated})\")\n",
    "                print(f\"[DEBUG] Day {day}: Skipped {day_outside_tolerance} (outside tolerance), {day_no_biomech_data} (no suitable biomech data)\")\n",
    "            resampled_list.append(day_df)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Day {day}: No valid biomech data created\")\n",
    "        interpolated_count += day_interpolated\n",
    "        exact_match_count += day_exact_match\n",
    "        outside_tolerance_count += day_outside_tolerance\n",
    "        no_biomech_data_count += day_no_biomech_data\n",
    "    \n",
    "    if resampled_list:\n",
    "        resampled_biomech = pd.concat(resampled_list, ignore_index=True)\n",
    "        total_processed = interpolated_count + exact_match_count + outside_tolerance_count + no_biomech_data_count\n",
    "        total_retained = interpolated_count + exact_match_count\n",
    "        retention_rate = (total_retained / total_processed * 100) if total_processed > 0 else 0\n",
    "        \n",
    "        print(f\"\\n=== Resampling Summary ===\")\n",
    "        print(f\"[INFO] EMG timestamps processed: {total_processed}\")\n",
    "        print(f\"[INFO] EMG timestamps retained: {total_retained} ({retention_rate:.2f}%)\")\n",
    "        print(f\"[INFO] Exact matches: {exact_match_count}\")\n",
    "        print(f\"[INFO] Interpolated points: {interpolated_count}\")\n",
    "        print(f\"[INFO] Skipped (outside tolerance): {outside_tolerance_count}\")\n",
    "        print(f\"[INFO] Skipped (no suitable biomech data): {no_biomech_data_count}\")\n",
    "        if debug:\n",
    "            mem_usage = resampled_biomech.memory_usage(deep=True).sum() / (1024**2)\n",
    "            print(f\"[DEBUG] Final resampled biomech shape = {resampled_biomech.shape}\")\n",
    "            print(f\"[DEBUG] Resampled biomech memory usage: {mem_usage:.2f} MB\")\n",
    "    else:\n",
    "        resampled_biomech = pd.DataFrame()\n",
    "        print(\"[WARNING] No biomech data could be resampled for any EMG timestamps within tolerance\")\n",
    "    \n",
    "    return resampled_biomech\n",
    "\n",
    "\n",
    "def strict_inner_join(emg_df, resampled_biomech, tolerance_ms=1, \n",
    "                     expected_exclusions=None,\n",
    "                     debug=False):\n",
    "    \"\"\"\n",
    "    Perform a strict merge_asof inner join using the EMG dataset as the base.\n",
    "    This version maintains a strict tolerance and only retains EMG rows that have\n",
    "    perfectly matched biomech data within the specified tolerance.\n",
    "    \n",
    "    Parameters:\n",
    "        emg_df (pd.DataFrame): EMG DataFrame.\n",
    "        resampled_biomech (pd.DataFrame): Resampled biomech DataFrame.\n",
    "        tolerance_ms (int): Tolerance in milliseconds for joining.\n",
    "        expected_exclusions (list, optional): Columns to exclude when checking for missing values.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Validated inner join DataFrame.\n",
    "    \"\"\"\n",
    "    if expected_exclusions is None:\n",
    "        expected_exclusions = [\"datetime\", \"biomech_datetime\"]\n",
    "\n",
    "    tolerance = pd.Timedelta(f\"{tolerance_ms}ms\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] strict_inner_join: EMG shape = {emg_df.shape}, Biomech shape = {resampled_biomech.shape}\")\n",
    "        print(f\"[DEBUG] strict_inner_join: Using tolerance = {tolerance}\")\n",
    "    \n",
    "    if resampled_biomech.empty:\n",
    "        print(\"[WARNING] No biomech data available for joining. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    CHUNK_SIZE = 10000\n",
    "    chunks = []\n",
    "    total_emg_rows = len(emg_df)\n",
    "    total_joined_rows = 0\n",
    "    total_no_match = 0\n",
    "    total_missing_data = 0\n",
    "    \n",
    "    emg_sorted = emg_df.sort_values('datetime')\n",
    "    biomech_sorted = resampled_biomech.sort_values('datetime')\n",
    "    \n",
    "    for i in range(0, len(emg_sorted), CHUNK_SIZE):\n",
    "        emg_chunk = emg_sorted.iloc[i:i+CHUNK_SIZE].copy()\n",
    "        chunk_size = len(emg_chunk)\n",
    "        if debug and i > 0:\n",
    "            print(f\"[DEBUG] strict_inner_join: Processing chunk {i//CHUNK_SIZE + 1} of {(len(emg_sorted) + CHUNK_SIZE - 1)//CHUNK_SIZE} ({chunk_size} rows)\")\n",
    "        \n",
    "        min_ts = emg_chunk['datetime'].min() - tolerance\n",
    "        max_ts = emg_chunk['datetime'].max() + tolerance\n",
    "        biomech_subset = biomech_sorted[\n",
    "            (biomech_sorted['datetime'] >= min_ts) & \n",
    "            (biomech_sorted['datetime'] <= max_ts)\n",
    "        ].copy()\n",
    "        \n",
    "        if biomech_subset.empty:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] strict_inner_join: No biomech data for EMG chunk {i//CHUNK_SIZE + 1}\")\n",
    "            total_no_match += chunk_size\n",
    "            continue\n",
    "        \n",
    "        chunk_joined = pd.merge_asof(\n",
    "            emg_chunk,\n",
    "            biomech_subset,\n",
    "            on=\"datetime\",\n",
    "            direction=\"nearest\",\n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        expected_cols = [col for col in biomech_subset.columns if col not in expected_exclusions]\n",
    "        before_drop = len(chunk_joined)\n",
    "        valid_chunk = chunk_joined.dropna(subset=expected_cols)\n",
    "        dropped_missing = before_drop - len(valid_chunk)\n",
    "        total_missing_data += dropped_missing\n",
    "        \n",
    "        if debug and dropped_missing > 0:\n",
    "            print(f\"[DEBUG] strict_inner_join: Dropped {dropped_missing} rows in chunk {i//CHUNK_SIZE + 1} due to missing biomech data\")\n",
    "        \n",
    "        if not valid_chunk.empty:\n",
    "            valid_chunk[\"time_difference\"] = (valid_chunk[\"datetime\"] - valid_chunk[\"biomech_datetime\"]).abs()\n",
    "            # Only retain rows strictly within the tolerance\n",
    "            valid_within_tolerance = valid_chunk[valid_chunk[\"time_difference\"] <= tolerance]\n",
    "            dropped_tolerance = len(valid_chunk) - len(valid_within_tolerance)\n",
    "            total_no_match += dropped_tolerance\n",
    "            if debug and dropped_tolerance > 0:\n",
    "                print(f\"[DEBUG] strict_inner_join: Dropped {dropped_tolerance} rows exceeding tolerance in chunk {i//CHUNK_SIZE + 1}\")\n",
    "            if not valid_within_tolerance.empty:\n",
    "                chunks.append(valid_within_tolerance)\n",
    "                total_joined_rows += len(valid_within_tolerance)\n",
    "    \n",
    "    if chunks:\n",
    "        valid_join = pd.concat(chunks, ignore_index=True)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] strict_inner_join: Final join shape = {valid_join.shape}\")\n",
    "            print(f\"[DEBUG] strict_inner_join: Time difference statistics (seconds):\")\n",
    "            print(valid_join[\"time_difference\"].dt.total_seconds().describe())\n",
    "        \n",
    "        retention_rate = (total_joined_rows / total_emg_rows) * 100\n",
    "        print(f\"\\n=== Join Summary ===\")\n",
    "        print(f\"[INFO] Total EMG rows: {total_emg_rows}\")\n",
    "        print(f\"[INFO] Successfully joined rows: {total_joined_rows} ({retention_rate:.2f}%)\")\n",
    "        print(f\"[INFO] Rows with no matching biomech data: {total_no_match}\")\n",
    "        print(f\"[INFO] Rows with missing required biomech columns: {total_missing_data}\")\n",
    "        return valid_join\n",
    "    else:\n",
    "        print(\"[WARNING] No valid joined rows could be created with the specified strict tolerance\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    biomech_path=None\n",
    "    emg_path=None\n",
    "    output_dir=None\n",
    "    debug=True\n",
    "    \"\"\"\n",
    "    Main function to run the EMG-Biomech inner join workflow with strict tolerance.\n",
    "    \n",
    "    Parameters:\n",
    "        biomech_path (str): Path to biomech data file.\n",
    "        emg_path (str): Path to EMG data file.\n",
    "        output_dir (str): Directory for output files.\n",
    "        debug (bool): If True, prints detailed debug information.\n",
    "    \"\"\"\n",
    "    # Use default paths if not provided\n",
    "    if biomech_path is None:\n",
    "        biomech_path = \"../../data/processed/ml_datasets/granular/granular_joint_details.parquet\"\n",
    "    if emg_path is None:\n",
    "        emg_path = \"../../data/processed/combined_emg_data.parquet\"\n",
    "    if output_dir is None:\n",
    "        output_dir = \"../../data/processed/ml_datasets\"\n",
    "    \n",
    "    # Configurable parameters – strict tolerance version\n",
    "    tolerance_ms = 1         # Strict tolerance in milliseconds for joining\n",
    "    buffer_minutes = 30      # Buffer for filtering biomech data around EMG data ranges\n",
    "    \n",
    "    print(f\"[INFO] === Configuration ===\")\n",
    "    print(f\"[INFO] Join tolerance: {tolerance_ms}ms (strict)\")\n",
    "    print(f\"[INFO] Buffer around EMG data: {buffer_minutes} minutes\")\n",
    "\n",
    "    # ---------------- Load Data ----------------\n",
    "    biomech_df = load_biomech_data(biomech_path, debug=debug)\n",
    "    emg_df = load_emg_data(emg_path, debug=debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n[DEBUG] Biomech DataFrame columns and null counts:\")\n",
    "        for col in biomech_df.columns:\n",
    "            print(f\" - {col}: {biomech_df[col].isnull().sum()} nulls\")\n",
    "        print(f\"[DEBUG] Total number of columns: {len(biomech_df.columns)}\")\n",
    "        print(f\"[DEBUG] Total number of null values: {biomech_df.isnull().sum().sum()}\")\n",
    "    \n",
    "    print(\"\\n[INFO] Performing initial biomech dataset checks...\")\n",
    "    if biomech_df.isnull().sum().sum() > 0:\n",
    "        print(\"[INFO] Dropping rows with null values from biomech data...\")\n",
    "        biomech_df = biomech_df.dropna()\n",
    "    print(f\"[INFO] Biomech data now has {len(biomech_df)} rows.\")\n",
    "\n",
    "    # Drop unnecessary columns from EMG data\n",
    "    emg_columns_to_drop = [\n",
    "        'EMG 1 (mV) - FDS', 'ACC X (G) - FDS', 'ACC Y (G) - FDS', \n",
    "        'ACC Z (G) - FDS','GYRO X (deg/s) - FDS','GYRO Y (deg/s) - FDS', \n",
    "        'GYRO Z (deg/s) - FDS','ACC X (G) - FCU','ACC Y (G) - FCU',\n",
    "        'ACC Z (G) - FCU','GYRO X (deg/s) - FCU','GYRO Y (deg/s) - FCU',\n",
    "        'GYRO Z (deg/s) - FCU','ACC X (G) - FCR','ACC Y (G) - FCR',\n",
    "        'ACC Z (G) - FCR','GYRO X (deg/s) - FCR','GYRO Y (deg/s) - FCR',\n",
    "        'GYRO Z (deg/s) - FCR'\n",
    "    ]\n",
    "    existing_cols_to_drop = [col for col in emg_columns_to_drop if col in emg_df.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        print(f\"[INFO] Dropping {len(existing_cols_to_drop)} unnecessary columns from EMG data\")\n",
    "        emg_df = emg_df.drop(columns=existing_cols_to_drop)\n",
    "    \n",
    "    total_nulls = emg_df.isnull().sum().sum()\n",
    "    if total_nulls > 0:\n",
    "        print(f\"[INFO] Dropping {total_nulls} null values from EMG data...\")\n",
    "        emg_df = emg_df.dropna()\n",
    "    print(f\"[INFO] EMG data now has {len(emg_df)} rows.\")\n",
    "\n",
    "    # ---------------- Compute Time Steps & Sort ----------------\n",
    "    biomech_df = compute_time_steps(biomech_df, debug=debug)\n",
    "    emg_df = compute_time_steps(emg_df, debug=debug)\n",
    "    biomech_df, emg_df = sort_dataframes(biomech_df, emg_df, debug=debug)\n",
    "\n",
    "    # ---------------- Filter Biomech Data by EMG Date Ranges ----------------\n",
    "    filtered_biomech = filter_biomech_by_emg_range(biomech_df, emg_df, buffer_minutes=buffer_minutes, debug=debug)\n",
    "    \n",
    "    # ---------------- Analyze Temporal Alignment ----------------\n",
    "    print(\"\\n[INFO] Analyzing temporal alignment between EMG and biomech data...\")\n",
    "    alignment_stats = analyze_temporal_alignment(filtered_biomech, emg_df, debug=debug)\n",
    "    if alignment_stats['viable_emg'] == 0:\n",
    "        print(\"[ERROR] No EMG points found within the strict tolerance of biomech data.\")\n",
    "        print(\"[INFO] Consider analyzing the temporal distribution of your data and trying again.\")\n",
    "\n",
    "    \n",
    "    # ---------------- Selective Resampling ----------------\n",
    "    print(\"\\n[INFO] Performing selective resampling with strict tolerance...\")\n",
    "    resampled_biomech = selective_resample_biomech_by_emg(\n",
    "        filtered_biomech, \n",
    "        emg_df, \n",
    "        tolerance_ms=tolerance_ms,\n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    if resampled_biomech.empty:\n",
    "        print(\"[ERROR] Resampling produced no usable data. No viable EMG-biomech pairs within tolerance.\")\n",
    "\n",
    "    \n",
    "    resampled_biomech[\"biomech_datetime\"] = resampled_biomech[\"datetime\"].copy()\n",
    "    resampled_biomech = resampled_biomech.rename(\n",
    "        columns={col: f\"{col}_biomech\" for col in resampled_biomech.columns \n",
    "                 if col not in [\"datetime\", \"biomech_datetime\", \"is_interpolated\"]}\n",
    "    )\n",
    "\n",
    "    # ---------------- Strict Inner Join ----------------\n",
    "    print(\"\\n[INFO] Performing strict inner join...\")\n",
    "    joined_df = strict_inner_join(\n",
    "        emg_df, \n",
    "        resampled_biomech, \n",
    "        tolerance_ms=tolerance_ms,\n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    if joined_df.empty:\n",
    "        print(\"[ERROR] Join produced no valid rows with the strict tolerance.\")\n",
    "\n",
    "    \n",
    "    final_join_path = f\"{output_dir}/final_inner_join_emg_biomech_data.parquet\"\n",
    "    save_dataframe(joined_df, final_join_path, \"Final strict inner join dataset\", debug=debug)\n",
    "    \n",
    "    sample_rows = joined_df.head(5)\n",
    "    sample_path = f\"{output_dir}/sample_emg_biomech_data.parquet\"\n",
    "    save_dataframe(sample_rows, sample_path, \"Sample rows from final joined dataset\", debug=debug)\n",
    "    \n",
    "    deep_analysis(joined_df, debug=debug)\n",
    "    \n",
    "    counts = resampled_biomech[\"is_interpolated\"].value_counts()\n",
    "    interp_count = counts.get(True, 0)\n",
    "    original_count = counts.get(False, 0)\n",
    "    print(f\"[INFO] Interpolated datapoints: {interp_count}; Original datapoints: {original_count}\")\n",
    "    print(\"\\n[INFO] Process completed successfully.\")\n",
    "    # return joined_df\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks into the pitch phases to understand them better:\n",
    "\n",
    "INFO:root:Aggregated Phase Duration Statistics:\n",
    "INFO:root:Phase: Follow Through -> Avg: 27.623s, Min: 2.036s***, Max: 124.441s****, Std: 22.539s*******\n",
    "INFO:root:Phase: Wind-Up -> Avg: 0.802s, Min: 0.587s, Max: 0.936s, Std: 0.085s\n",
    "INFO:root:Phase: Stride -> Avg: 0.628s, Min: 0.567s, Max: 0.752s, Std: 0.047s\n",
    "INFO:root:Phase: Arm Cocking -> Avg: 0.103s, Min: 0.085s, Max: 0.119s, Std: 0.009s\n",
    "INFO:root:Phase: Arm Acceleration -> Avg: 0.047s, Min: 0.005s, Max: 0.060s, Std: 0.015s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "logger.info(\"Loading final inner join EMG biomech dataset...\")\n",
    "df = pd.read_parquet('../../data/processed/ml_datasets/final_inner_join_emg_biomech_data 2.parquet')\n",
    "\n",
    "# Print all columns\n",
    "logger.info(\"\\nColumns in dataset:\")\n",
    "logger.info(df.columns.tolist())\n",
    "\n",
    "# Check unique pitch types\n",
    "logger.info(\"\\nUnique pitch types:\")\n",
    "logger.info(df['pitch_type_biomech'].unique())\n",
    "logger.info(\"\\nPitch type counts:\")\n",
    "logger.info(df['pitch_type_biomech'].value_counts())\n",
    "\n",
    "# Get basic statistics for all columns\n",
    "logger.info(\"\\nDataset description:\")\n",
    "logger.info(df.describe())\n",
    "logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "# # Check ball_released_biomech column\n",
    "# logger.info(\"\\nChecking ball_released_biomech column:\")\n",
    "# logger.info(f\"Number of True values: {df['ball_released_biomech'].sum()}\")\n",
    "# logger.info(f\"Number of False values: {(~df['ball_released_biomech']).sum()}\")\n",
    "# logger.info(f\"Number of null values: {df['ball_released_biomech'].isnull().sum()}\")\n",
    "# logger.info(\"\\nValue distribution:\")\n",
    "# logger.info(df['ball_released_biomech'].value_counts(normalize=True))\n",
    "\n",
    "# Basic checks for zeros and nulls in pitch_phase_biomech\n",
    "logger.info(\"\\nBasic phase checks:\")\n",
    "zeros_in_phase = (df['pitch_phase_biomech'] == 0).sum()\n",
    "logger.info(f\"Number of zeros in pitch_phase: {zeros_in_phase}\")\n",
    "\n",
    "nulls_in_phase = df['pitch_phase_biomech'].isnull().sum()\n",
    "logger.info(f\"Number of nulls in pitch_phase_biomech: {nulls_in_phase}\")\n",
    "\n",
    "logger.info(\"\\nValue counts in pitch_phase:\")\n",
    "logger.info(df['pitch_phase_biomech'].value_counts())\n",
    "\n",
    "# Check for nulls in all columns\n",
    "null_counts = df.isnull().sum()\n",
    "columns_with_nulls = null_counts[null_counts > 0]\n",
    "if len(columns_with_nulls) > 0:\n",
    "    logger.info(\"\\nColumns containing null values:\")\n",
    "    logger.info(columns_with_nulls)\n",
    "else:\n",
    "    logger.info(\"\\nNo null values found in any columns\")\n",
    "\n",
    "# Ensure datetime is parsed and data is sorted globally\n",
    "df['datetime'] = pd.to_datetime(df['biomech_datetime'])\n",
    "df = df.sort_values('datetime')\n",
    "\n",
    "logger.info(\"\\nChecking for overlapping pitch phases, time gaps and phase durations per trial...\")\n",
    "\n",
    "# Group data by trial\n",
    "trial_groups = df.groupby('trial_biomech')\n",
    "\n",
    "# Loop over each trial\n",
    "for trial, trial_data in trial_groups:\n",
    "    # Sort data for the current trial by datetime\n",
    "    trial_data = trial_data.sort_values('datetime')\n",
    "    \n",
    "    # Create a summary of phases: start and end times per phase\n",
    "    phase_summary = trial_data.groupby('pitch_phase_biomech').agg(\n",
    "        phase_start=('datetime', 'min'),\n",
    "        phase_end=('datetime', 'max')\n",
    "    ).reset_index().sort_values('phase_start')\n",
    "    \n",
    "    # Calculate duration for each phase\n",
    "    phase_summary['duration'] = phase_summary['phase_end'] - phase_summary['phase_start']\n",
    "    \n",
    "    logger.info(f\"\\nTrial: {trial}\")\n",
    "    logger.info(\"Phase timeline summary:\")\n",
    "    for idx, row in phase_summary.iterrows():\n",
    "        logger.info(f\"  - {row['pitch_phase_biomech']}: start: {row['phase_start']}, end: {row['phase_end']}, duration: {row['duration']}\")\n",
    "    \n",
    "    # Check for overlaps and gaps between consecutive phases\n",
    "    for i in range(1, len(phase_summary)):\n",
    "        prev_phase = phase_summary.iloc[i - 1]\n",
    "        curr_phase = phase_summary.iloc[i]\n",
    "        \n",
    "        # If previous phase's end is after the current phase's start, there's an overlap.\n",
    "        if prev_phase['phase_end'] > curr_phase['phase_start']:\n",
    "            overlap_duration = prev_phase['phase_end'] - curr_phase['phase_start']\n",
    "            logger.info(\n",
    "                f\"Overlap detected between '{prev_phase['pitch_phase_biomech']}' and \"\n",
    "                f\"'{curr_phase['pitch_phase_biomech']}': {overlap_duration} (duration)\"\n",
    "            )\n",
    "        else:\n",
    "            # Otherwise, compute the gap (time between phases)\n",
    "            gap_duration = curr_phase['phase_start'] - prev_phase['phase_end']\n",
    "            logger.info(\n",
    "                f\"Gap between '{prev_phase['pitch_phase_biomech']}' and \"\n",
    "                f\"'{curr_phase['pitch_phase_biomech']}': {gap_duration} (time between phases)\"\n",
    "            )\n",
    "    \n",
    "    # Optional: Within each trial, also check if any phase internally has overlapping points from a different phase.\n",
    "    phase_groups = trial_data.groupby('pitch_phase_biomech')\n",
    "    for phase in trial_data['pitch_phase_biomech'].unique():\n",
    "        phase_data = phase_groups.get_group(phase)\n",
    "        phase_start = phase_data['datetime'].min()\n",
    "        phase_end = phase_data['datetime'].max()\n",
    "        overlapping = {}\n",
    "        for other_phase in trial_data['pitch_phase_biomech'].unique():\n",
    "            if other_phase == phase:\n",
    "                continue\n",
    "            other_phase_data = phase_groups.get_group(other_phase)\n",
    "            overlap_points = other_phase_data[\n",
    "                (other_phase_data['datetime'] >= phase_start) &\n",
    "                (other_phase_data['datetime'] <= phase_end)\n",
    "            ]\n",
    "            if not overlap_points.empty:\n",
    "                overlapping[other_phase] = len(overlap_points)\n",
    "        if overlapping:\n",
    "            logger.info(f\"In trial {trial}, phase '{phase}' overlaps with:\")\n",
    "            for other_phase, count in overlapping.items():\n",
    "                logger.info(f\"  - '{other_phase}': {count} overlapping points\")\n",
    "        else:\n",
    "            logger.info(f\"In trial {trial}, phase '{phase}' has no overlapping points with other phases.\")\n",
    "\n",
    "\n",
    "import logging\n",
    "import statistics\n",
    "from datetime import timedelta\n",
    "from functools import wraps\n",
    "\n",
    "def require_array_type(func):\n",
    "    \"\"\"\n",
    "    Decorator that asserts the second argument (usually the data input)\n",
    "    has a 'shape' attribute, i.e. is array-like.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # args[1] should be the phase_data input for _align_phase\n",
    "        if not hasattr(args[1], 'shape'):\n",
    "            raise TypeError(f\"Function {func.__name__} requires array-like input\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# --- Modified/Added Functions ---\n",
    "\n",
    "def print_phase_duration_statistics(durations_by_phase):\n",
    "    \"\"\"\n",
    "    Computes and logs statistics for each phase:\n",
    "        - Average duration\n",
    "        - Minimum duration\n",
    "        - Maximum duration\n",
    "        - Standard deviation of durations\n",
    "    Also computes the overall min and max standard deviation across phases.\n",
    "    \n",
    "    Parameters:\n",
    "        durations_by_phase (dict): Dictionary where keys are phase names and values\n",
    "                                   are lists of durations (in seconds).\n",
    "    \"\"\"\n",
    "    logging.info(\"Aggregated Phase Duration Statistics:\")\n",
    "    std_devs = {}  # To store standard deviation for each phase\n",
    "\n",
    "    for phase, durations in durations_by_phase.items():\n",
    "        avg_duration = statistics.mean(durations)\n",
    "        min_duration = min(durations)\n",
    "        max_duration = max(durations)\n",
    "        # Calculate standard deviation; if only one duration, define std as 0.\n",
    "        std_duration = statistics.stdev(durations) if len(durations) > 1 else 0.0\n",
    "        std_devs[phase] = std_duration\n",
    "\n",
    "        logging.info(\n",
    "            f\"Phase: {phase} -> Avg: {avg_duration:.3f}s, \"\n",
    "            f\"Min: {min_duration:.3f}s, Max: {max_duration:.3f}s, Std: {std_duration:.3f}s\"\n",
    "        )\n",
    "\n",
    "    if std_devs:\n",
    "        overall_min_std = min(std_devs.values())\n",
    "        \n",
    "\n",
    "def process_trials(trials):\n",
    "    \"\"\"\n",
    "    Process each trial to print phase timeline summaries.\n",
    "    At the same time, collect phase durations for later statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        trials (list): List of trial objects/dictionaries. Each trial is expected\n",
    "                       to have a 'phases' dictionary with keys as phase names and\n",
    "                       values as a dict containing 'start' and 'end' (datetime objects).\n",
    "    \"\"\"\n",
    "    # Dictionary to hold durations for each phase (in seconds)\n",
    "    durations_by_phase = {}\n",
    "    \n",
    "    for trial in trials:\n",
    "        trial_id = trial.get(\"id\", \"Unknown\")\n",
    "        logging.info(f\"Trial: {trial_id}\")\n",
    "        logging.info(\"Phase timeline summary:\")\n",
    "        \n",
    "        # Assume trial['phases'] is a dict like: { 'Phase Name': {'start': dt, 'end': dt}, ... }\n",
    "        phases = trial.get(\"phases\", {})\n",
    "        # Sort phases by start time (if needed)\n",
    "        sorted_phases = sorted(phases.items(), key=lambda item: item[1]['start'])\n",
    "        \n",
    "        for phase, timeline in sorted_phases:\n",
    "            start = timeline['start']\n",
    "            end = timeline['end']\n",
    "            # Calculate duration as a timedelta and then convert to seconds\n",
    "            duration_td = end - start\n",
    "            duration_sec = duration_td.total_seconds()\n",
    "            logging.info(f\"  - {phase}: start: {start}, end: {end}, duration: {duration_td}\")\n",
    "            \n",
    "            # Collect durations\n",
    "            durations_by_phase.setdefault(phase, []).append(duration_sec)\n",
    "        \n",
    "        # (If your code also prints gap information, leave that logic unchanged)\n",
    "        # For example:\n",
    "        # logging.info(f\"Gap between 'Phase1' and 'Phase2': {gap_td} (time between phases)\")\n",
    "    \n",
    "    # After processing all trials, print aggregated statistics:\n",
    "    print_phase_duration_statistics(durations_by_phase)\n",
    "\n",
    "def process_trials_from_df(df, trial_id_col='trial_biomech', phase_col='pitch_phase_biomech', datetime_col='datetime'):\n",
    "    durations_by_phase = {}\n",
    "    trial_groups = df.groupby(trial_id_col)\n",
    "    \n",
    "    for trial_id, trial_data in trial_groups:\n",
    "        logging.info(f\"Trial: {trial_id}\")\n",
    "        logging.info(\"Phase timeline summary:\")\n",
    "        \n",
    "        # Group by phase within this trial\n",
    "        phase_summary = trial_data.groupby(phase_col).agg(\n",
    "            phase_start=(datetime_col, 'min'),\n",
    "            phase_end=(datetime_col, 'max')\n",
    "        ).reset_index().sort_values('phase_start')\n",
    "        \n",
    "        for idx, row in phase_summary.iterrows():\n",
    "            start = row['phase_start']\n",
    "            end = row['phase_end']\n",
    "            duration_td = end - start\n",
    "            duration_sec = duration_td.total_seconds()\n",
    "            logging.info(f\"  - {row[phase_col]}: start: {start}, end: {end}, duration: {duration_td}\")\n",
    "            \n",
    "            durations_by_phase.setdefault(row[phase_col], []).append(duration_sec)\n",
    "        \n",
    "        # (Optional: Check for overlaps and gaps as before)\n",
    "    \n",
    "    print_phase_duration_statistics(durations_by_phase)\n",
    "\n",
    "\n",
    "\n",
    "process_trials_from_df(df, phase_col='pitch_phase_biomech')\n",
    "\n",
    "\n",
    "# Count unique trials\n",
    "num_trials = df['trial_biomech'].nunique()\n",
    "logger.info(f\"\\nTotal number of unique trials: {num_trials}\")\n",
    "\n",
    "# Optional: Show distribution of data points across trials\n",
    "trial_counts = df.groupby('trial_biomech').size()\n",
    "logger.info(\"\\nData points per trial:\")\n",
    "logger.info(f\"Mean: {trial_counts.mean():.2f}\")\n",
    "logger.info(f\"Min: {trial_counts.min()}\")\n",
    "logger.info(f\"Max: {trial_counts.max()}\")\n",
    "logger.info(f\"Std: {trial_counts.std():.2f}\")\n",
    "\n",
    "# Optional: Show trial distribution by session\n",
    "session_trial_counts = df.groupby('session_biomech')['trial_biomech'].nunique()\n",
    "logger.info(\"\\nTrials per session:\")\n",
    "logger.info(session_trial_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checks combine and filter Checks for EMG Signals\n",
    "\n",
    "Create Dashboard for monitoring this data flow so we are ensuring now too many low signals and we ensure we capture all the high signals\n",
    "\n",
    "\n",
    "Summarized emg dataset show KPI's and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch, butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "# Ensure fft functions are imported for FFT analysis\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Butterworth band-pass filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    if low <= 0 or high >= 1 or low >= high:\n",
    "        raise ValueError(f\"Check critical frequencies: got low={lowcut}, high={highcut} Hz.\")\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Band-pass filter function (missing from your original script)\n",
    "def bandpass_filter(emg_signal, lowcut=20, highcut=500, fs=1000, order=4):\n",
    "    print(f\"Applying bandpass filter from {lowcut}-{highcut} Hz\")\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    return filtfilt(b, a, emg_signal)\n",
    "\n",
    "\n",
    "def compute_emg_features(emg_signal, fs):\n",
    "    \"\"\"\n",
    "    Compute common EMG features from a single segment of raw EMG data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    emg_signal : 1D numpy array\n",
    "        Raw EMG data samples for a single time window/segment.\n",
    "    fs : float\n",
    "        Sampling frequency of the EMG data (in Hz).\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        A dictionary of computed EMG features:\n",
    "          Time-Domain:\n",
    "\n",
    "'RMS': Root Mean Square\n",
    "'MAV': Mean Absolute Value\n",
    "'IEMG': Integrated EMG\n",
    "'Variance': Signal Variance\n",
    "'ZeroCrossings': Zero Crossing Count\n",
    "'WaveformLength': Waveform Length\n",
    "          Frequency-Domain:\n",
    "\n",
    "'MeanFrequency': Mean Frequency of the power spectrum\n",
    "'MedianFrequency': Median Frequency of the power spectrum\n",
    "'PeakFrequency': Frequency at which power is highest\n",
    "    \"\"\"\n",
    "    # ---------------------\n",
    "    # 1. TIME-DOMAIN FEATURES\n",
    "    # ---------------------\n",
    "    # (A) Root Mean Square (RMS)\n",
    "    rms_val = np.sqrt(np.mean(emg_signal**2))\n",
    "    # (B) Mean Absolute Value (MAV)\n",
    "    mav_val = np.mean(np.abs(emg_signal))\n",
    "    # (C) Integrated EMG (IEMG)\n",
    "    iemg_val = np.sum(np.abs(emg_signal))\n",
    "    # (D) Variance\n",
    "    # ddof=1 makes it the sample variance\n",
    "    var_val = np.var(emg_signal, ddof=1)\n",
    "    # (E) Zero Crossing Count (ZCR)\n",
    "    zero_crossings = 0\n",
    "    for i in range(len(emg_signal) - 1):\n",
    "        if np.sign(emg_signal[i]) != np.sign(emg_signal[i+1]):\n",
    "            zero_crossings += 1\n",
    "    # (F) Waveform Length (WL)\n",
    "    waveform_length = np.sum(np.abs(np.diff(emg_signal)))\n",
    "    # ---------------------\n",
    "    # 2. FREQUENCY-DOMAIN FEATURES\n",
    "    # ---------------------\n",
    "    #\n",
    "    # We can use the Welch method to estimate the Power Spectral Density (PSD).\n",
    "    # 'welch' splits the signal into sub-segments to get a smoother PSD estimate.\n",
    "    freqs, psd = welch(emg_signal, fs=fs, nperseg=len(emg_signal))\n",
    "    # psd[i] is the power at frequency freqs[i].\n",
    "    # (A) Mean Frequency (MNF)\n",
    "    # = (sum of frequency * PSD) / (sum of PSD)\n",
    "    total_power = np.sum(psd)\n",
    "    mean_frequency = np.sum(freqs * psd) / total_power if total_power > 0 else 0\n",
    "    # (B) Median Frequency (MDF)\n",
    "    # = frequency where half of the total power lies below it\n",
    "    cumulative_power = np.cumsum(psd)\n",
    "    half_power = cumulative_power[-1] / 2.0\n",
    "    median_freq_idx = np.where(cumulative_power >= half_power)[0][0]\n",
    "    median_frequency = freqs[median_freq_idx]\n",
    "    # (C) Peak Frequency\n",
    "    # = frequency at which the PSD is maximum\n",
    "    peak_idx = np.argmax(psd)\n",
    "    peak_frequency = freqs[peak_idx]\n",
    "    # ---------------------\n",
    "    # 3. COLLECT AND RETURN FEATURES\n",
    "    # ---------------------\n",
    "    features = {\n",
    "        # Time-Domain\n",
    "        'RMS': rms_val,\n",
    "        'MAV': mav_val,\n",
    "        'IEMG': iemg_val,\n",
    "        'Variance': var_val,\n",
    "        'ZeroCrossings': zero_crossings,\n",
    "        'WaveformLength': waveform_length,\n",
    "        # Frequency-Domain\n",
    "        'MeanFrequency': mean_frequency,\n",
    "        'MedianFrequency': median_frequency,\n",
    "        'PeakFrequency': peak_frequency\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# Butterworth high-pass filter to remove low-frequency noise below the specified cutoff\n",
    "def butter_highpass(lowcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    print(f\"Normalized lowcut frequency: {low}\")  # Debugging\n",
    "    if low <= 0:\n",
    "        raise ValueError(f\"Critical frequency must be greater than 0. Got lowcut={low}.\")\n",
    "    b, a = butter(order, low, btype='high')\n",
    "    return b, a\n",
    "\n",
    "def highpass_filter(emg_signal, lowcut=20, fs=1000, order=4):\n",
    "    print(f\"Applying highpass filter with lowcut={lowcut} Hz, fs={fs} Hz\")  # Debugging\n",
    "    b, a = butter_highpass(lowcut, fs, order)\n",
    "    return filtfilt(b, a, emg_signal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example preprocessing and feature extraction pipeline\n",
    "def process_and_compute_features(emg_signal, fs, highpass=True):\n",
    "    # 1. Remove excessive zeros (if needed, set threshold)\n",
    "    if (emg_signal == 0).sum() > len(emg_signal) * 0.1:  # If more than 10% zeros, discard\n",
    "        return None\n",
    "    \n",
    "    # If highpass is True, apply highpass filter, otherwise, skip it\n",
    "    if highpass:\n",
    "        # 2. High-pass filter to remove low-frequency noise (20 Hz range)\n",
    "        # Collect statistics before filtering\n",
    "        pre_filter_stats = {\n",
    "            'min': np.min(emg_signal),\n",
    "            'max': np.max(emg_signal),\n",
    "            'mean': np.mean(emg_signal),\n",
    "            'std': np.std(emg_signal),\n",
    "        }\n",
    "\n",
    "        # Apply the high-pass filter\n",
    "        filtered_signal = highpass_filter(emg_signal, lowcut=20, fs=fs)\n",
    "\n",
    "        # Collect statistics after filtering\n",
    "        post_filter_stats = {\n",
    "            'min': np.min(filtered_signal),\n",
    "            'max': np.max(filtered_signal),\n",
    "            'mean': np.mean(filtered_signal),\n",
    "            'std': np.std(filtered_signal),\n",
    "        }\n",
    "\n",
    "        # Compare pre and post filter statistics\n",
    "        print(\"Pre-filter statistics:\", pre_filter_stats)\n",
    "        print(\"Post-filter statistics:\", post_filter_stats)\n",
    "\n",
    "    else:\n",
    "        # If no filter is applied, use the original signal\n",
    "        filtered_signal = emg_signal\n",
    "\n",
    "    # 3. Compute features from filtered (or original) signal\n",
    "    return compute_emg_features(filtered_signal, fs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#---------------Checks---------\n",
    "def plot_psd_before_and_after(emg_signal, fs, lowcut=20, highcut=500):\n",
    "    \"\"\"\n",
    "    Plot Power Spectral Density (PSD) before and after filtering to compare frequency content.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    emg_signal : 1D numpy array\n",
    "        Raw EMG signal data.\n",
    "    fs : float\n",
    "        Sampling frequency of the EMG data (in Hz).\n",
    "    lowcut : float\n",
    "        Lowcut frequency for bandpass filter (Hz).\n",
    "    highcut : float\n",
    "        Highcut frequency for bandpass filter (Hz).\n",
    "    \"\"\"\n",
    "    # Plot before filtering\n",
    "    freqs_before, psd_before = welch(emg_signal, fs=fs, nperseg=len(emg_signal))\n",
    "    \n",
    "    # Apply bandpass filter\n",
    "    filtered_signal = bandpass_filter(emg_signal, lowcut=lowcut, highcut=highcut, fs=fs)\n",
    "    \n",
    "    # Plot after filtering\n",
    "    freqs_after, psd_after = welch(filtered_signal, fs=fs, nperseg=len(filtered_signal))\n",
    "    \n",
    "    # Plot both PSDs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.semilogy(freqs_after, psd_after, label='After Filtering', color='red')\n",
    "    plt.semilogy(freqs_before, psd_before, label='Before Filtering', color='blue')\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power Spectral Density (uV^2/Hz)\")\n",
    "    plt.title(\"Power Spectral Density Before and After Filtering\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Load EMG pitch data for validation not for use really\n",
    "emg_pitch_data = pd.read_parquet('../data/processed/emg_pitch_data_processed.parquet')\n",
    "# print(\"\\nEMG Pitch Data Columns and Unique Values:\")\n",
    "# for col in emg_pitch_data.columns:\n",
    "#     print(f\"\\n{col}:\")\n",
    "#     print(f\"Unique values: {emg_pitch_data[col].unique()}\")\n",
    "    \n",
    "\n",
    "\n",
    "# Get all EMG columns\n",
    "emg_signal_columns = ['EMG 1 (mV) - FDS (81770)', 'EMG 1 (mV) - FCU (81728)', 'EMG 1 (mV) - FCR (81745)']\n",
    "print(\"EMG Signals here ===========\", emg_signal_columns)\n",
    "fs = 1000  # Sampling frequency (adjust based on your setup)\n",
    "\n",
    "# Print detailed information about each EMG column\n",
    "for column in emg_signal_columns:\n",
    "    print(f\"\\n=== Analysis of {column} ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(f\"Number of values: {len(emg_pitch_data[column])}\")\n",
    "    print(f\"Number of null values: {emg_pitch_data[column].isnull().sum()}\")\n",
    "    print(f\"Data type: {emg_pitch_data[column].dtype}\")\n",
    "    \n",
    "    # Value analysis\n",
    "    print(\"\\nValue Analysis:\")\n",
    "    first_value = emg_pitch_data[column].iloc[0]\n",
    "    print(f\"First value type: {type(first_value)}\")\n",
    "    print(f\"Shape (if array): {np.shape(first_value) if hasattr(first_value, 'shape') else 'N/A'}\")\n",
    "    print(f\"First few values: {emg_pitch_data[column].head().values}\")\n",
    "    \n",
    "    # Numerical statistics if possible\n",
    "    try:\n",
    "        print(\"\\nNumerical Statistics:\")\n",
    "        print(f\"Mean: {emg_pitch_data[column].mean():.4f}\")\n",
    "        print(f\"Std: {emg_pitch_data[column].std():.4f}\")\n",
    "        print(f\"Min: {emg_pitch_data[column].min():.4f}\")\n",
    "        print(f\"Max: {emg_pitch_data[column].max():.4f}\")\n",
    "        \n",
    "        # Check for potential issues\n",
    "        print(\"\\nPotential Issues:\")\n",
    "        print(f\"Number of zeros: {(emg_pitch_data[column] == 0).sum()}\")\n",
    "        print(f\"Number of unique values: {emg_pitch_data[column].nunique()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not compute numerical statistics: {str(e)}\")\n",
    "\n",
    "# Process each EMG signal\n",
    "features_dict = {}\n",
    "# Create separate feature dictionaries for filtered and unfiltered data\n",
    "filtered_features = {}\n",
    "unfiltered_features = {}\n",
    "\n",
    "for signal_col in emg_signal_columns:\n",
    "    # Extract raw EMG signal\n",
    "    emg_signal = emg_pitch_data[signal_col].values\n",
    "    \n",
    "    # Process with highpass filter\n",
    "    filtered = process_and_compute_features(emg_signal, fs, highpass=True)\n",
    "    if filtered:\n",
    "        filtered_features[signal_col] = filtered\n",
    "    else:\n",
    "        print(f\"Filtered signal {signal_col} was discarded due to excessive zeros.\")\n",
    "        \n",
    "    # Process without highpass filter \n",
    "    unfiltered = process_and_compute_features(emg_signal, fs, highpass=False)\n",
    "    if unfiltered:\n",
    "        unfiltered_features[signal_col] = unfiltered\n",
    "    else:\n",
    "        print(f\"Unfiltered signal {signal_col} was discarded due to excessive zeros.\")\n",
    "\n",
    "# Compare and validate features\n",
    "for signal_col in emg_signal_columns:\n",
    "    print(f\"\\nValidation for {signal_col}:\")\n",
    "    \n",
    "    if signal_col in filtered_features and signal_col in unfiltered_features:\n",
    "        filtered = filtered_features[signal_col]\n",
    "        unfiltered = unfiltered_features[signal_col]\n",
    "        \n",
    "        print(\"\\nFeature comparison (filtered vs unfiltered):\")\n",
    "        for feature in filtered.keys():\n",
    "            filtered_val = filtered[feature]\n",
    "            unfiltered_val = unfiltered[feature]\n",
    "            diff = abs(filtered_val - unfiltered_val)\n",
    "            print(f\"{feature}:\")\n",
    "            print(f\"  Filtered: {filtered_val:.4f}\")\n",
    "            print(f\"  Unfiltered: {unfiltered_val:.4f}\") \n",
    "            print(f\"  Difference: {diff:.4f}\")\n",
    "            \n",
    "            # Flag large differences\n",
    "            if diff > 0.5 * abs(filtered_val):\n",
    "                print(\"  WARNING: Large difference between filtered and unfiltered values\")\n",
    "    \n",
    "    # Visualize frequency content\n",
    "    signal = emg_pitch_data[signal_col].values\n",
    "    N = len(signal)\n",
    "    T = 1.0 / fs\n",
    "    x = np.linspace(0.0, N*T, N, endpoint=False)\n",
    "    \n",
    "    # FFT analysis\n",
    "    yf = fft(signal)\n",
    "    xf = fftfreq(N, T)[:N//2]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(f\"Frequency Spectrum - {signal_col}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Power spectral density comparison\n",
    "    plot_psd_before_and_after(signal, fs=fs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_fatigue_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
