{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Input folder '../../data/raw/three_sensored_emg_data/' does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 611\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;66;03m# Run the module when executed as a script.\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;66;03m# Set debug=True for detailed output, or False for minimal output.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m     processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/raw/three_sensored_emg_data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Specify your input folder path here.\u001b[39;49;00m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/processed/emg_pitch_data_processed.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify your output file path here.\u001b[39;49;00m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 573\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(debug, input_folder, output_file)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# Ensure input folder exists.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(input_folder):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput folder \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# Find all CSV files in the folder.\u001b[39;00m\n\u001b[0;32m    576\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Input folder '../../data/raw/three_sensored_emg_data/' does not exist."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Read sensor data with metadata and update column names with sensor group and mode identifiers\n",
    "########################################\n",
    "def read_sensor_data_with_metadata(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Reads the sensor CSV file that contains metadata in the first five lines,\n",
    "    a header in line 5, a sample rate row in line 6, and sensor data from line 7 onward.\n",
    "    \n",
    "    For updated data (with multiple sensors: e.g., FDS, FCU, FCR), it parses:\n",
    "      - Line 3: Sensor group identifiers (e.g., \"FDS (81770), , ... , FCU (81728), ..., FCR (81745)\")\n",
    "      - Line 4: Sensor mode information for each group.\n",
    "    \n",
    "    It then updates the header (line 5) by appending the sensor group to each column name.\n",
    "    The sensor mode is loaded into metadata but not appended to the column name.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the CSV file.\n",
    "      debug (bool): If True, prints detailed debug output.\n",
    "    \n",
    "    Returns:\n",
    "      df (pd.DataFrame): DataFrame with sensor data, updated column names, metadata columns, and a \"Timestamp\" column.\n",
    "      metadata (dict): Dictionary of parsed metadata.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        all_lines = f.readlines()\n",
    "    \n",
    "    metadata = {}\n",
    "    # --- Parse first 3 lines (common for both formats) ---\n",
    "    # Line 0: Application\n",
    "    line = all_lines[0].strip()\n",
    "    if ',' in line:\n",
    "        key, value = line.split(',', 1)\n",
    "        metadata[key.strip().rstrip(':')] = value.strip()\n",
    "    else:\n",
    "        metadata['Application'] = line\n",
    "\n",
    "    # Line 1: Date/Time\n",
    "    line = all_lines[1].strip()\n",
    "    if ',' in line:\n",
    "        key, value = line.split(',', 1)\n",
    "        metadata[key.strip().rstrip(':')] = value.strip()\n",
    "    else:\n",
    "        metadata['Date/Time'] = line\n",
    "\n",
    "    # Line 2: Collection Length (seconds)\n",
    "    line = all_lines[2].strip()\n",
    "    if ',' in line:\n",
    "        key, value = line.split(',', 1)\n",
    "        metadata[key.strip().rstrip(':')] = value.strip()\n",
    "    else:\n",
    "        metadata['Collection Length (seconds)'] = line\n",
    "\n",
    "    # --- Determine dataset type (updated or legacy) ---\n",
    "    sensor_group_line = all_lines[3].strip()\n",
    "    sensor_mode_line = all_lines[4].strip()\n",
    "    if ',' in sensor_group_line and len(sensor_group_line.split(',')) > 1:\n",
    "        # Updated dataset detected\n",
    "        sensor_group_tokens = [token.strip() for token in sensor_group_line.split(',')]\n",
    "        # Propagate non-empty values forward\n",
    "        sensor_groups = []\n",
    "        last = None\n",
    "        for token in sensor_group_tokens:\n",
    "            if token:\n",
    "                last = token\n",
    "            sensor_groups.append(last if last is not None else \"\")\n",
    "        # Similarly for sensor modes (line 4)\n",
    "        sensor_mode_tokens = [token.strip() for token in sensor_mode_line.split(',')]\n",
    "        sensor_modes = []\n",
    "        last_mode = None\n",
    "        for token in sensor_mode_tokens:\n",
    "            if token:\n",
    "                last_mode = token\n",
    "            sensor_modes.append(last_mode if last_mode is not None else \"\")\n",
    "        # Store these in metadata\n",
    "        metadata['SensorGroups'] = sensor_groups\n",
    "        metadata['SensorModes'] = sensor_modes\n",
    "        if debug:\n",
    "            print(f\"[read_sensor_data_with_metadata] SensorGroups: {sensor_groups}\")\n",
    "            print(f\"[read_sensor_data_with_metadata] SensorModes: {sensor_modes}\")\n",
    "    else:\n",
    "        # Legacy dataset: use line 3 and 4 as single values.\n",
    "        metadata['Sensor'] = sensor_group_line\n",
    "        metadata['Sensor Mode'] = sensor_mode_line\n",
    "\n",
    "    # --- Header row for sensor data is on line 5 in both cases ---\n",
    "    header_line = all_lines[5].strip()\n",
    "    original_col_names = [col.strip() for col in header_line.split(',')]\n",
    "    \n",
    "    # If updated dataset, update column names by appending only sensor group.\n",
    "    if 'SensorGroups' in metadata:\n",
    "        if len(metadata['SensorGroups']) >= len(original_col_names):\n",
    "            new_col_names = []\n",
    "            for i, col in enumerate(original_col_names):\n",
    "                group = metadata['SensorGroups'][i]\n",
    "                new_col_names.append(f\"{col} - {group}\")\n",
    "            if debug:\n",
    "                print(\"[read_sensor_data_with_metadata] New column names set (updated dataset, sensor group only).\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"[read_sensor_data_with_metadata] Warning: Not enough sensor group entries; using original column names.\")\n",
    "            new_col_names = original_col_names\n",
    "    else:\n",
    "        new_col_names = original_col_names\n",
    "\n",
    "    # Read the sensor data (starting at line 7)\n",
    "    data_str = ''.join(all_lines[7:])\n",
    "    df = pd.read_csv(StringIO(data_str), header=None, names=new_col_names)\n",
    "\n",
    "    # Add metadata columns to the DataFrame (except sensor group and mode lists)\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['SensorGroups', 'SensorModes']:\n",
    "            df[key] = value\n",
    "\n",
    "    # Create a running Timestamp column\n",
    "    collection_length = float(metadata.get('Collection Length (seconds)', 0))\n",
    "    start_time = pd.to_datetime(metadata.get('Date/Time', None))\n",
    "    num_samples = len(df)\n",
    "    time_offsets = np.linspace(0, collection_length, num_samples)\n",
    "    df['Timestamp'] = start_time + pd.to_timedelta(time_offsets, unit='s')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[read_sensor_data_with_metadata] Final DataFrame shape: {df.shape}\")\n",
    "        print(f\"[read_sensor_data_with_metadata] Final column names: {df.columns.tolist()}\")\n",
    "    else:\n",
    "        print(\"read_sensor_data_with_metadata completed.\")\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compute EMG extreme flag using a fixed time window\n",
    "########################################\n",
    "def compute_emg_extreme_flag_window(df, window_time=1.3, column='EMG 1 (mV)', \n",
    "                                    threshold_high=1.0, threshold_low=-0.5, debug=False):\n",
    "    \"\"\"\n",
    "    Computes a flag for each row indicating whether, within a fixed time window\n",
    "    around the current row, there is at least one EMG value above 'threshold_high' and\n",
    "    one below 'threshold_low'.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing a 'Timestamp' column.\n",
    "      window_time (float): Time window in seconds.\n",
    "      column (str): Column name with EMG values.\n",
    "      threshold_high (float): High threshold.\n",
    "      threshold_low (float): Low threshold.\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      pd.Series: Series of 0/1 flags.\n",
    "    \"\"\"\n",
    "    time_diffs = df['Timestamp'].diff().dropna().dt.total_seconds()\n",
    "    median_dt = time_diffs.median() if not time_diffs.empty else 0\n",
    "    frame_count = int(round(window_time / median_dt)) if median_dt > 0 else 0\n",
    "    if debug:\n",
    "        print(f\"[compute_emg_extreme_flag_window] Using a time window of {window_time} sec (~{frame_count} frames)\")\n",
    "    \n",
    "    flags = []\n",
    "    timestamps = df['Timestamp']\n",
    "    values = df[column]\n",
    "    for idx, current_time in timestamps.items():\n",
    "        start_time = current_time - pd.Timedelta(seconds=window_time)\n",
    "        end_time = current_time + pd.Timedelta(seconds=window_time)\n",
    "        window_mask = (timestamps >= start_time) & (timestamps <= end_time)\n",
    "        window_values = values[window_mask]\n",
    "        flag = int((window_values > threshold_high).any() and (window_values < threshold_low).any())\n",
    "        flags.append(flag)\n",
    "    \n",
    "    flag_series = pd.Series(flags, index=df.index)\n",
    "    if debug:\n",
    "        print(f\"[compute_emg_extreme_flag_window] Output flags shape: {flag_series.shape}\")\n",
    "    else:\n",
    "        print(\"compute_emg_extreme_flag_window completed.\")\n",
    "    return flag_series\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compute EMG extreme flag using a dynamic time window\n",
    "########################################\n",
    "def compute_emg_extreme_flag_dynamic_window(df, column='EMG 1 (mV)', threshold_high=1.0, \n",
    "                                              threshold_low=-0.5, debug=False):\n",
    "    \"\"\"\n",
    "    Computes a dynamic extreme flag for each row by first determining a fixed-window flag,\n",
    "    then adjusting the time window based on the nearest extreme events.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame containing a 'Timestamp' column.\n",
    "      column (str): Column name with EMG values.\n",
    "      threshold_high (float): High threshold.\n",
    "      threshold_low (float): Low threshold.\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      pd.Series: Series of 0/1 flags.\n",
    "    \"\"\"\n",
    "    fixed_flags = compute_emg_extreme_flag_window(df, window_time=1.3, column=column, \n",
    "                                                   threshold_high=threshold_high, threshold_low=threshold_low, debug=debug)\n",
    "    extreme_times = df.loc[fixed_flags == 1, 'Timestamp']\n",
    "    extreme_time_array = extreme_times.sort_values().values  # numpy array of timestamps\n",
    "    \n",
    "    new_flags = []\n",
    "    dynamic_windows = []  # store δ (in seconds) for each row\n",
    "    timestamps = df['Timestamp']\n",
    "    values = df[column]\n",
    "    \n",
    "    for idx, current_time in timestamps.items():\n",
    "        current_time_np = np.datetime64(current_time)\n",
    "        pos = np.searchsorted(extreme_time_array, current_time_np)\n",
    "        prev_extreme = extreme_time_array[pos - 1] if pos > 0 else None\n",
    "        next_extreme = extreme_time_array[pos] if pos < len(extreme_time_array) else None\n",
    "        \n",
    "        if prev_extreme is not None and next_extreme is not None:\n",
    "            delta_prev = (current_time_np - prev_extreme).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "            delta_next = (next_extreme - current_time_np).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "            delta_sec = min(delta_prev, delta_next)\n",
    "        elif prev_extreme is not None:\n",
    "            delta_sec = (current_time_np - prev_extreme).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "        elif next_extreme is not None:\n",
    "            delta_sec = (next_extreme - current_time_np).astype('timedelta64[ns]').astype(float) / 1e9\n",
    "        else:\n",
    "            delta_sec = 0\n",
    "        dynamic_windows.append(delta_sec)\n",
    "        \n",
    "        start_time = current_time - pd.Timedelta(seconds=delta_sec)\n",
    "        end_time = current_time + pd.Timedelta(seconds=delta_sec)\n",
    "        window_mask = (timestamps >= start_time) & (timestamps <= end_time)\n",
    "        window_values = values[window_mask]\n",
    "        flag = int((window_values > threshold_high).any() and (window_values < threshold_low).any())\n",
    "        new_flags.append(flag)\n",
    "    \n",
    "    new_flags_series = pd.Series(new_flags, index=df.index)\n",
    "    avg_dynamic_window = np.mean(dynamic_windows) if dynamic_windows else 0\n",
    "    if debug:\n",
    "        print(f\"[compute_emg_extreme_flag_dynamic_window] Average dynamic window size: {avg_dynamic_window:.2f} sec\")\n",
    "    else:\n",
    "        print(\"compute_emg_extreme_flag_dynamic_window completed.\")\n",
    "    return new_flags_series\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Build global min/max dictionary\n",
    "########################################\n",
    "def build_global_min_max(df, columns_to_analyze, debug=False):\n",
    "    \"\"\"\n",
    "    Builds and returns a dictionary mapping each column to its global min and max.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame.\n",
    "      columns_to_analyze (list): List of column names.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      dict: Dictionary with min and max for each column.\n",
    "    \"\"\"\n",
    "    global_dict = {}\n",
    "    for col in columns_to_analyze:\n",
    "        global_dict[col] = {'min': df[col].min(), 'max': df[col].max()}\n",
    "    if debug:\n",
    "        print(f\"[build_global_min_max] Global min/max for columns: {global_dict}\")\n",
    "    else:\n",
    "        print(\"build_global_min_max completed.\")\n",
    "    return global_dict\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compute window metrics\n",
    "########################################\n",
    "def compute_window_metrics(window_df, columns_to_analyze, global_min_max=None, debug=False):\n",
    "    \"\"\"\n",
    "    Computes metrics (average, min, max) for a given window of data.\n",
    "    \n",
    "    Parameters:\n",
    "      window_df (pd.DataFrame): DataFrame slice.\n",
    "      columns_to_analyze (list): List of column names.\n",
    "      global_min_max (dict, optional): Dictionary for global min/max comparison.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      dict: Dictionary of computed metrics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for col in columns_to_analyze:\n",
    "        w_min = window_df[col].min()\n",
    "        w_max = window_df[col].max()\n",
    "        w_avg = window_df[col].mean()\n",
    "        stats[f'avg_{col}'] = w_avg\n",
    "        stats[f'min_{col}'] = w_min\n",
    "        stats[f'max_{col}'] = w_max\n",
    "        if global_min_max is not None and col in global_min_max:\n",
    "            g_min = global_min_max[col]['min']\n",
    "            g_max = global_min_max[col]['max']\n",
    "            stats[f'is_global_min_{col}'] = (w_min == g_min)\n",
    "            stats[f'is_global_max_{col}'] = (w_max == g_max)\n",
    "    if debug:\n",
    "        print(f\"[compute_window_metrics] Computed stats: {stats}\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Analyze spikes in a given column\n",
    "########################################\n",
    "def analyze_spikes(df, col, window=50, global_min_max=None, debug=False):\n",
    "    \"\"\"\n",
    "    Analyzes spikes in a given column using a specified window.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame.\n",
    "      col (str): Column name.\n",
    "      window (int): Window size (number of rows) around the spike.\n",
    "      global_min_max (dict, optional): Dictionary for global min/max.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with computed spike metrics.\n",
    "    \"\"\"\n",
    "    spike_flag_col = f'{col}_spike_flag'\n",
    "    flagged_indices = df.index[df[spike_flag_col] == 1]\n",
    "    results = []\n",
    "    columns_to_analyze = [\n",
    "        'EMG 1 (mV)', 'ACC X (G)', 'ACC Y (G)', 'ACC Z (G)',\n",
    "        'GYRO X (deg/s)', 'GYRO Y (deg/s)', 'GYRO Z (deg/s)'\n",
    "    ]\n",
    "    \n",
    "    for idx in flagged_indices:\n",
    "        start_idx = max(0, idx - window)\n",
    "        end_idx = min(len(df) - 1, idx + window)\n",
    "        window_df = df.loc[start_idx:end_idx]\n",
    "        window_stats = compute_window_metrics(window_df, columns_to_analyze, global_min_max=global_min_max, debug=debug)\n",
    "        window_stats['spike_index'] = idx\n",
    "        window_stats['spike_column'] = col\n",
    "        window_stats['spike_value'] = df.loc[idx, col]\n",
    "        window_stats['window_start'] = start_idx\n",
    "        window_stats['window_end'] = end_idx\n",
    "        results.append(window_stats)\n",
    "    if debug:\n",
    "        print(f\"[analyze_spikes] Processed {len(flagged_indices)} spikes for column {col}.\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Compare spike windows from EMG and ACC/GYRO\n",
    "########################################\n",
    "def compare_spike_windows(emg_spikes_df, acc_gyro_spikes_df, debug=False):\n",
    "    \"\"\"\n",
    "    Compares spike windows from EMG and ACC/GYRO and returns merged information.\n",
    "    \n",
    "    Parameters:\n",
    "      emg_spikes_df (pd.DataFrame): DataFrame from analyze_spikes for EMG.\n",
    "      acc_gyro_spikes_df (pd.DataFrame): DataFrame from analyze_spikes for ACC/GYRO.\n",
    "      debug (bool): If True, prints debug info.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, emg_row in emg_spikes_df.iterrows():\n",
    "        emg_win_start = emg_row['window_start']\n",
    "        emg_win_end = emg_row['window_end']\n",
    "        overlapping_spikes = acc_gyro_spikes_df[\n",
    "            (acc_gyro_spikes_df['spike_index'] >= emg_win_start) &\n",
    "            (acc_gyro_spikes_df['spike_index'] <= emg_win_end)\n",
    "        ]\n",
    "        for j, spike_row in overlapping_spikes.iterrows():\n",
    "            merged_dict = {\n",
    "                'emg_spike_index': emg_row['spike_index'],\n",
    "                'emg_spike_value': emg_row['spike_value'],\n",
    "                'acc_gyro_spike_index': spike_row['spike_index'],\n",
    "                'acc_gyro_spike_column': spike_row['spike_column'],\n",
    "                'emg_window_avg': emg_row['avg_EMG 1 (mV)'],\n",
    "                'acc_window_avg': spike_row.get('avg_ACC X (G)', None)\n",
    "            }\n",
    "            rows.append(merged_dict)\n",
    "    if debug:\n",
    "        print(f\"[compare_spike_windows] Merged {len(rows)} overlapping spike events.\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Mark throwing motion based on extreme flag windows\n",
    "########################################\n",
    "def mark_throwing_motion(df, extreme_flag_col='EMG_extreme_flag', window_time=1.3, debug=False):\n",
    "    \"\"\"\n",
    "    Marks rows as part of the throwing motion based on extreme flag events.\n",
    "    \n",
    "    For each row where the specified extreme_flag_col is 1, mark all rows within ±(window_time/2) seconds \n",
    "    of that event's timestamp as part of the throwing motion by setting a new column 'ThrowingMotion' to 1.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): DataFrame with a 'Timestamp' column.\n",
    "      extreme_flag_col (str): The column name that holds the extreme flag.\n",
    "      window_time (float): Total duration (in seconds) for the throwing motion window.\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Copy of the DataFrame with an added 'ThrowingMotion' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['ThrowingMotion'] = 0\n",
    "    half_window = window_time / 2  # e.g., 0.65 seconds for a 1.3-second window\n",
    "    \n",
    "    # Debug: Show the number of extreme events.\n",
    "    extreme_events = df.loc[df[extreme_flag_col] == 1, 'Timestamp']\n",
    "    if debug:\n",
    "        print(f\"[mark_throwing_motion] Found {len(extreme_events)} extreme events. Using half window = {half_window} sec.\")\n",
    "    \n",
    "    # Mark rows within the window of each extreme event.\n",
    "    for t in extreme_events:\n",
    "        start = t - pd.Timedelta(seconds=half_window)\n",
    "        end = t + pd.Timedelta(seconds=half_window)\n",
    "        mask = (df['Timestamp'] >= start) & (df['Timestamp'] <= end)\n",
    "        df.loc[mask, 'ThrowingMotion'] = 1\n",
    "        if debug:\n",
    "            print(f\"[mark_throwing_motion] Marking event at {t} (window: {start} to {end}).\")\n",
    "    \n",
    "    if debug:\n",
    "        total_marked = df['ThrowingMotion'].sum()\n",
    "        print(f\"[mark_throwing_motion] Total rows marked as ThrowingMotion: {total_marked}\")\n",
    "    else:\n",
    "        print(\"mark_throwing_motion completed.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "########################################\n",
    "# Function: Process a single CSV file\n",
    "########################################\n",
    "def process_file(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Processes a single sensor CSV file:\n",
    "      - Reads the file and its metadata.\n",
    "      - Performs cleaning and type conversion.\n",
    "      - Computes various flags and metrics.\n",
    "      - Marks throwing motion.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the CSV file.\n",
    "      debug (bool): If True, prints detailed debug output.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n[process_file] Processing file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Step 1: Read data and metadata.\n",
    "    df, metadata = read_sensor_data_with_metadata(file_path, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] DataFrame shape after reading: {df.shape}\")\n",
    "    else:\n",
    "        print(\"Data read completed.\")\n",
    "\n",
    "    # Step 2: Display minimal summary if in debug mode.\n",
    "    if debug:\n",
    "        print(f\"[process_file] Descriptive Statistics:\\n{df.describe()}\")\n",
    "        print(f\"[process_file] Data types:\\n{df.dtypes}\")\n",
    "    else:\n",
    "        print(\"Basic summary displayed.\")\n",
    "\n",
    "    # Step 3: Dynamically identify numeric sensor columns.\n",
    "    base_names = ['ACC X (G)', 'ACC Y (G)', 'ACC Z (G)', \n",
    "                  'GYRO X (deg/s)', 'GYRO Y (deg/s)', 'GYRO Z (deg/s)']\n",
    "    numeric_cols = []\n",
    "    for base in base_names:\n",
    "        matches = [col for col in df.columns if col.startswith(base)]\n",
    "        numeric_cols.extend(matches)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Identified numeric sensor columns: {numeric_cols}\")\n",
    "\n",
    "    # Clean data: Remove rows with blank numeric values.\n",
    "    mask = df[numeric_cols].apply(lambda col: col.astype(str).str.strip() == '').any(axis=1)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Rows with blank numeric values: {mask.sum()}\")\n",
    "    df = df[~mask]\n",
    "\n",
    "    # Convert identified numeric columns to numeric type.\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "        except Exception as e:\n",
    "            print(f\"[process_file] Error converting column {col}: {e}\")\n",
    "            raise\n",
    "    if debug:\n",
    "        print(f\"[process_file] Data shape after cleaning: {df.shape}\")\n",
    "\n",
    "    # Step 4: (Optional) Subset data; here we use the full dataset.\n",
    "    print(f\"[process_file] Data subset: {df.shape[0]} rows (full data used).\")\n",
    "    \n",
    "    # (Optional) Compute overall min/max summary.\n",
    "    min_max_df = pd.DataFrame({'min': df.min(), 'max': df.max()})\n",
    "    if debug:\n",
    "        print(f\"[process_file] Overall min/max summary:\\n{min_max_df}\")\n",
    "    else:\n",
    "        print(\"Min/Max summary computed.\")\n",
    "\n",
    "    # Step 5: Create spike flags for ACC/GYRO columns.\n",
    "    for col in numeric_cols:\n",
    "        spike_flag_col = f'{col}_spike_flag'\n",
    "        df[spike_flag_col] = ((df[col] > 1) | (df[col] < -0.5)).astype(int)\n",
    "    print(\"Spike flags for ACC/GYRO created.\")\n",
    "\n",
    "    # Create spike flag for EMG (value > 1.0).\n",
    "    emg_base = 'EMG 1 (mV)'\n",
    "    emg_matches = [col for col in df.columns if col.startswith(emg_base)]\n",
    "    if emg_matches:\n",
    "        emg_col = emg_matches[0]\n",
    "    else:\n",
    "        raise KeyError(f\"No column found starting with '{emg_base}'\")\n",
    "    emg_spike_flag_col = f'{emg_col}_spike_flag'\n",
    "    df[emg_spike_flag_col] = (df[emg_col] > 1.0).astype(int)\n",
    "    \n",
    "    # Additional EMG flags.\n",
    "    df['EMG_high_flag'] = (df[emg_col] > 1.0).astype(int)\n",
    "    df['EMG_low_flag'] = (df[emg_col] < -0.5).astype(int)\n",
    "    if debug:\n",
    "        print(f\"[process_file] EMG_high_flag, EMG_low_flag added. Count >1.0: {df['EMG_high_flag'].sum()}, \"\n",
    "              f\"Count <-0.5: {df['EMG_low_flag'].sum()}\")\n",
    "\n",
    "    # Step 6: Compute fixed-window extreme flag for EMG.\n",
    "    df['EMG_extreme_flag'] = compute_emg_extreme_flag_window(df, window_time=1.3, column=emg_col, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Fixed-window extreme flag count: {df['EMG_extreme_flag'].sum()}\")\n",
    "\n",
    "    # Step 7: Count unique extreme events in fixed window.\n",
    "    unique_extreme_count = ((df['EMG_extreme_flag'] == 1) &\n",
    "                            (df['EMG_extreme_flag'].shift(1).fillna(0) != 1)).sum()\n",
    "    if debug:\n",
    "        print(f\"[process_file] Unique extreme events (fixed window): {unique_extreme_count}\")\n",
    "\n",
    "    # Step 8: Compute dynamic-window extreme flag for EMG.\n",
    "    df['EMG_extreme_flag_dynamic'] = compute_emg_extreme_flag_dynamic_window(df, column=emg_col, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] Dynamic-window extreme flag count: {df['EMG_extreme_flag_dynamic'].sum()}\")\n",
    "        unique_dynamic_extreme_count = ((df['EMG_extreme_flag_dynamic'] == 1) &\n",
    "                                        (df['EMG_extreme_flag_dynamic'].shift(1).fillna(0) != 1)).sum()\n",
    "        print(f\"[process_file] Unique extreme events (dynamic window): {unique_dynamic_extreme_count}\")\n",
    "\n",
    "    # Step 9: Mark throwing motion based on fixed-window extreme flags.\n",
    "    df = mark_throwing_motion(df, extreme_flag_col='EMG_extreme_flag', window_time=1.3, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"[process_file] ThrowingMotion rows count: {df['ThrowingMotion'].sum()}\")\n",
    "\n",
    "    print(\"File processing completed.\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "########################################\n",
    "# Main function: Process all files in a folder and output a single Parquet file\n",
    "########################################\n",
    "def main(debug=False, input_folder='./data/raw/', output_file='./data/processed/processed_pitch_data.parquet'):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the specified folder, stacks them into one DataFrame,\n",
    "    and writes the output to a Parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "      debug (bool): If True, prints detailed debug information.\n",
    "      input_folder (str): Folder containing the CSV files.\n",
    "      output_file (str): Path for the output Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: Final processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure input folder exists.\n",
    "    if not os.path.isdir(input_folder):\n",
    "        raise FileNotFoundError(f\"Input folder '{input_folder}' does not exist.\")\n",
    "    \n",
    "    # Find all CSV files in the folder.\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the input folder.\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[main] Found {len(csv_files)} CSV files in '{input_folder}'.\")\n",
    "    else:\n",
    "        print(f\"Found {len(csv_files)} CSV file(s).\")\n",
    "\n",
    "    processed_dfs = []\n",
    "    for file in csv_files:\n",
    "        df = process_file(file, debug=debug)\n",
    "        # Optionally add a column to indicate source file.\n",
    "        df['SourceFile'] = os.path.basename(file)\n",
    "        processed_dfs.append(df)\n",
    "    \n",
    "    # Stack all DataFrames (row-wise).\n",
    "    final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "    if debug:\n",
    "        print(f\"[main] Final stacked DataFrame shape: {final_df.shape}\")\n",
    "    else:\n",
    "        print(\"All files processed and stacked.\")\n",
    "\n",
    "    # Save final DataFrame to Parquet.\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    final_df.to_parquet(output_file, index=False)\n",
    "    print(f\"Final processed data saved to: {output_file}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Run the module when executed as a script.\n",
    "if __name__ == \"__main__\":\n",
    "    # Set debug=True for detailed output, or False for minimal output.\n",
    "    processed_df = main(\n",
    "        debug=True,\n",
    "        input_folder='../../data/raw/three_sensored_emg_data/',         # Specify your input folder path here.\n",
    "        output_file='../../data/processed/emg_pitch_data_processed.parquet'  # Specify your output file path here.\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_fatigue_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
